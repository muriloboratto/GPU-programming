{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549339c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Accelerating CUDA C++ Applications with Multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1cc8f",
   "metadata": {},
   "source": [
    "Welcome to the material _Accelerating CUDA C++ Applications with Multiple GPUs_ for `Instruction certification application` in NVIDIA.\n",
    "\n",
    "## Personal Informations:\n",
    "\n",
    "I will introduce myself\n",
    "\n",
    "- My name is **Murilo Boratto**;\n",
    "- I am **Computer Engenier**;\n",
    "- I had my PHD ten years ago, in **HPC** (High Performance Computing) topic at the **Universitat Politècnica de València**, in Spain;\n",
    "- Actually I am working in the **Supercomputing Center SENAI CIMATEC**, in the city Salvador of Bahia in Brasil. My function in this center is **Research Leader in HPC and Parallel Computing projects**. My research line is inside of **accelerating computing**, basically is optimize and portable sequential code for GPU environments;\n",
    "- I have skills in GPU programming using the APIs CUDA, OpenACC, CUDAWARE, NCCL, MPI, OpenMP ...\n",
    "\n",
    "This is the summary about me!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b3f132-1faa-49c6-9a96-dc278afc400f",
   "metadata": {},
   "source": [
    "## `JupyterLab`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01335b1b-3b91-4962-8991-590e066e2486",
   "metadata": {},
   "source": [
    "### What is the JupyterLab?\n",
    "\n",
    "The JupyterLab is an **`interface for interactive computing that can use the browser for compile and execute parallel codes`**. It extends the functionality of traditional Jupyter Notebooks by offering a more flexible and versatile interface, combining various components such as notebooks, terminals, text editors, and data file viewers in one unified workspace on the browser.\n",
    "\n",
    "### What is the advantages for use the JupyterLab?\n",
    "\n",
    "1. Flexible Interface\n",
    "\n",
    "2. Rich Text and Code Editing\n",
    "\n",
    "### Which areas can I use JupyterLab?\n",
    "\n",
    "1. HPC, Data Science, and Machine Learning\n",
    "\n",
    "2. Software Development and Prototyping codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804aec0",
   "metadata": {},
   "source": [
    "## `CUDA thread hierarchy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56417956",
   "metadata": {},
   "source": [
    "### What is CUDA thread hierarchy?\n",
    "\n",
    "In CUDA, the thread hierarchy plays a critical role in organizing the parallel execution of code and optimizing performance. It provides a **`structured way to launch and manage threads in a grid of blocks`**, which are executed on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f01b1b3-5a37-40dc-842c-594191a9ab1e",
   "metadata": {},
   "source": [
    "### What does the CUDA thread hierarchy consist of?\n",
    "\n",
    "The CUDA thread hierarchy consists of three main levels: **threads**, **blocks**, and **grids**. Each level serves a specific purpose in organizing the parallel execution of tasks:\n",
    "\n",
    "▶ **Thread**: **The smallest unit of execution**. Each thread executes a kernel function independently. Threads can access their own local memory, shared memory within the block, and global memory accessible by all threads.\n",
    "\n",
    "▶ **Block**: **Threads are grouped into blocks**. Each block contains a fixed number of threads, typically arranged in 1D, 2D, or 3D. Blocks execute independently, making it possible for different blocks to be executed on different streaming multiprocessors (SMs). Threads within a block can cooperate with each other using shared memory, which is fast and shared among threads in the same block.\n",
    "\n",
    "▶ **Grid**: **A grid is composed of multiple blocks**. When a kernel is launched, it is executed across all the threads in all the blocks of the grid. Like blocks, grids can also be one-, two-, or three-dimensional, providing flexibility in the organization of threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0055a042-7277-43f0-9260-a74eadb1ac05",
   "metadata": {},
   "source": [
    "### Sample example of CUDA Thread Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8844f98-2517-4541-b6b8-fd0695dc9703",
   "metadata": {},
   "source": [
    "#### Matrix Multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2e9ea-d6db-4e10-af25-3bf91d2c3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mm.cu\n",
    "#include <cuda.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <omp.h>\n",
    "\n",
    "__global__ void kernel(int *A, int *B, int *C, int size)\n",
    "{\n",
    "  int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  int j = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\n",
    "  if((i < size) && (j < size))\n",
    "    for(int k = 0; k < size; k++)\n",
    "       C[i * size + j] += A[i * size + k] * B[k * size + j];\n",
    "\n",
    "}\n",
    "\n",
    "void initializeMatrix(int *A, int size)\n",
    "{\n",
    "  for(int i = 0; i < size; i++)\n",
    "    for(int j = 0; j < size; j++)\n",
    "      A[i * size + j] = rand() % (10 - 1) * 1;\n",
    "}\n",
    "\n",
    "void printMatrix(int *A, int size)\n",
    "{\n",
    "  for(int i = 0; i < size; i++){\n",
    "    for(int j = 0; j < size; j++)\n",
    "      printf(\"%d\\t\", A[i * size + j]);\n",
    "    printf(\"\\n\");\n",
    "  }\n",
    "  printf(\"\\n\");\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv)\n",
    "{\n",
    "  if (argc < 3)\n",
    "  {\n",
    "    printf(\"%s [SIZE] [BLOCKSIZE]\\n\", argv[0]);\n",
    "    exit(-1);\n",
    "  }\n",
    "\n",
    "  int n = atoi(argv[1]);\n",
    "  int blockSize = atoi(argv[2]);\n",
    "  double t1, t2;\n",
    "\n",
    " //Memory Allocation in the Host\n",
    "  int  *A = (int *) malloc (sizeof(int)*n*n);\n",
    "  int  *B = (int *) malloc (sizeof(int)*n*n);\n",
    "  int  *C = (int *) malloc (sizeof(int)*n*n);\n",
    "\n",
    "  initializeMatrix(A, n);\n",
    "  initializeMatrix(B, n);\n",
    "\n",
    " //printMatrix(A, n);\n",
    " //printMatrix(B, n);\n",
    " //printMatrix(C, n);\n",
    "\n",
    " // Memory Allocation in the Device\n",
    "  int *d_A, *d_B, *d_C;\n",
    "  cudaMalloc((void **) &d_A, n * n * sizeof(int) ) ;\n",
    "  cudaMalloc((void **) &d_B, n * n * sizeof(int) ) ;\n",
    "  cudaMalloc((void **) &d_C, n * n * sizeof(int) ) ;\n",
    "\n",
    "  t1 = omp_get_wtime();\n",
    "\n",
    " // Copy of data from host to device\n",
    "  cudaMemcpy( d_A, A, n * n * sizeof(int), cudaMemcpyHostToDevice ) ;\n",
    "  cudaMemcpy( d_B, B, n * n * sizeof(int), cudaMemcpyHostToDevice ) ;\n",
    "  cudaMemcpy( d_C, C, n * n * sizeof(int), cudaMemcpyHostToDevice ) ;\n",
    "\n",
    " // 2D Computational Grid\n",
    "  dim3 dimGrid( (int) ceil( (float) n / (float) blockSize ), (int) ceil( (float) n / (float) blockSize ) );\n",
    "  dim3 dimBlock( blockSize, blockSize);\n",
    "\n",
    "            kernel<<<dimGrid, dimBlock>>>(A, B, C, n);\n",
    "\n",
    " // Copy of data from device to host\n",
    "  cudaMemcpy( C, d_C, n * n * sizeof(float), cudaMemcpyDeviceToHost ) ;\n",
    "\n",
    "  t2 = omp_get_wtime();\n",
    "\n",
    "  printf(\"%d\\t%f\\n\", n, t2-t1);\n",
    "\n",
    " //printMatrix(A, n);\n",
    " //printMatrix(B, n);\n",
    " //printMatrix(C, n);\n",
    "\n",
    "// Memory Allocation in the Device\n",
    " cudaFree(d_A) ;\n",
    " cudaFree(d_B) ;\n",
    " cudaFree(d_C) ;\n",
    "\n",
    "// Memory Allocation in the Host\n",
    " free(A);\n",
    " free(B);\n",
    " free(C);\n",
    "\n",
    " return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63356d35-934b-4468-b552-a2155c331ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc mm.cu -o mm -Xcompiler -fopenmp -O3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01376ebe-2094-41e2-b1ef-05a431c37f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./mm 10000 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d439036-1063-45f8-8282-4b1afd3e5fe7",
   "metadata": {},
   "source": [
    "**threadIdx.x**: Index of the thread within its block.\n",
    "\n",
    "**blockIdx.x**: Index of the block within the grid.\n",
    "\n",
    "**blockDim.x**: Number of threads in each block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676fc47f-4cfa-4274-86ea-2f205d745aab",
   "metadata": {},
   "source": [
    "### What are the advantages of the CUDA Thread Hierarchy?\n",
    "\n",
    "▶ **Scalability**: is the **`ability to efficiently handle increased workloads by adding more computational resources`**.\n",
    "\n",
    "▶ **Flexible Mapping**: the **`ability to assign computational tasks or processes to available resources`**.\n",
    "\n",
    "▶ **Efficient Memory Usage**: is the **`efficient use of the shared memory reducing the memory latency`**.\n",
    "\n",
    "The CUDA thread hierarchy is fundamental to understanding how to design and optimize GPU-accelerated programs. Properly leveraging this structure can lead to significant performance improvements in parallel computing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13333e4c-fd14-4992-a266-496591f7fff3",
   "metadata": {},
   "source": [
    "## `CUDA kernel`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31b19e-f2a4-48bf-839a-f2116451e1e3",
   "metadata": {},
   "source": [
    "### What is a CUDA kernel?\n",
    "\n",
    "**It is a function of the computation that executed on GPU resource**.  The execution a kernel, you launch it from the CPU code using a special syntax that specifies the number of blocks and threads. This launch configuration determines the overall parallelism, with each block containing multiple threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd96f6-6395-4654-8be0-66e913b581fe",
   "metadata": {},
   "source": [
    "## `Concurrent CUDA streams and their behaviors`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb61e72-2bcc-4a54-9704-6adefc2f8372",
   "metadata": {},
   "source": [
    "### What are Concurrent CUDA Streams?\n",
    "\n",
    "In CUDA, **streams are sequences of operations that are executed on the GPU in order**. Using multiple concurrent CUDA streams allows for overlapping kernel execution, memory transfers, and other operations, improving performance by enabling asynchronous execution. \n",
    "\n",
    "### How to Create and Use CUDA Streams?\n",
    "\n",
    "#### Creating a CUDA Stream:\n",
    "\n",
    "CUDA streams are created using the **cudaStreamCreate()** function and destroyed with cudaStreamDestroy().\n",
    "\n",
    "~~~c++\n",
    "cudaStream_t stream1, stream2;\n",
    "cudaStreamCreate(&stream1);\n",
    "cudaStreamCreate(&stream2);\n",
    "~~~\n",
    "\n",
    "#### Using CUDA Streams in Operations:\n",
    "\n",
    "When launching a kernel or performing memory transfers, you can specify the stream in which the operation should execute.\n",
    "\n",
    "~~~c++\n",
    "myKernel<<<numBlocks, threadsPerBlock, 0, stream1>>>(args...);\n",
    "cudaMemcpyAsync(dest, src, size, cudaMemcpyHostToDevice, stream2);\n",
    "~~~\n",
    "\n",
    "In the example above, myKernel runs in stream1, while the asynchronous memory copy happens in stream2. If resources allow, both can execute concurrently.\n",
    "\n",
    "\n",
    "#### Destroying a CUDA Stream:\n",
    "\n",
    "~~~c++\n",
    "cudaStreamDestroy(stream1);\n",
    "cudaStreamDestroy(stream2);\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63074024-7b2c-4d09-831b-59ef6abc9b78",
   "metadata": {},
   "source": [
    "## `Memory management and data transfer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd0d30-4b22-47c7-a58a-c5b30dec9a6f",
   "metadata": {},
   "source": [
    "Memory management and data transfers in a GPU environment using CUDA are crucial for achieving optimal performance in parallel computing applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95169346-96d8-4b9f-a1f5-4caa8032f03b",
   "metadata": {},
   "source": [
    "### What is the Memory management and data transfer on Multi-GPU systems?\n",
    "\n",
    "In GPU systems, the concept of Data Transfer Between Host and Devices **`refers to the movement of data between the CPU (host) and GPU (device) memory spaces`**. This process is necessary because the CPU and GPU typically have separate memory pools, and data that is processed by the GPU often originates from the CPU.\n",
    "\n",
    "### What are the types that is possible?\n",
    "\n",
    "- **Host-to-Device (H2D) and Device-to-Host (D2H) Transfers**: These are typically needed to initialize input data and retrieve results.\n",
    "\n",
    "- **Pinned Memory**: Related with **direct memory access (DMA)** transfers in the process.\n",
    "\n",
    "- **Unified Memory**: Introduced to simplify memory management, it allows for automatic migration of data between the host and device. This can be convenient, but manual management is often faster due to better control over **data locality**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c526522-17d2-4cea-a76d-15a323667878",
   "metadata": {},
   "source": [
    "### What is the concept Asynchronous Memory Transfers on GPU systems?\n",
    "\n",
    "Asynchronous Memory Transfers on GPU systems refer to the **ability to transfer data between the host and the device memory using overlapping concepts**. This allows the CPU to continue processing other tasks while the data is being moved, which can significantly improve overall system efficiency, especially in high-performance and real-time applications.\n",
    "\n",
    "\n",
    "- CUDA supports asynchronous data transfers using cudaMemcpyAsync(), which allows overlapping data transfers with computation. This is beneficial for hiding data transfer latency by using streams, which are queues of operations that can execute in parallel.\n",
    "\n",
    "- Stream Management: Multiple streams can be used to perform different tasks simultaneously, such as overlapping data transfers and kernel execution, leading to higher GPU utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd327529-6532-4d92-8215-b6474015af1a",
   "metadata": {},
   "source": [
    "### What are the Optimization Strategies for Memory Management?\n",
    "\n",
    "- Minimize Data Transfers\n",
    "- Use Asynchronous Transfers and Overlap Computation\n",
    "- Exploit Unified Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b69a4-5496-483c-8e29-9d0c33c3afcd",
   "metadata": {},
   "source": [
    "## `Data chunking strategy for multi-stream and multi-GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ec22e-26ed-4432-9e28-e71ef4f831f4",
   "metadata": {},
   "source": [
    "### What is the Data Chunking Strategy?\n",
    "\n",
    "The data chunking strategy for multi-stream and multi-GPU environments is a technique used to optimize data processing and parallelism in CUDA applications. This strategy involves **`dividing large datasets into smaller chunks that can be processed independently and in parallel across multiple CUDA streams or multiple GPUs`**. This approach maximizes hardware utilization and improves throughput by reducing idle times and overlapping computation with data transfers.\n",
    "\n",
    "In multi-GPU environments, multiple GPUs can be used simultaneously to process large datasets. By **distributing the data chunks across different GPUs**, the workload can be parallelized further, leveraging the combined computational power of all GPUs. Each GPU operates independently, and data chunking ensures that the workload is balanced across all GPUs. Properly distributing the chunks among GPUs is crucial to avoid bottlenecks and ensure even load distribution.\n",
    "\n",
    "Example: If I can $N$ data sets and $M$ GPUs, we can distribute $N/M$ to each GPU of the system. Using differents strategies, i.e., Static (equals chunks), Dynamic (small chunks and the asign is continuos), Hybrid, Out-of-core (the contents doesn't fit on the GPUs), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d450e2-9b71-43c6-a425-ea79e682cba0",
   "metadata": {},
   "source": [
    "## `Copy/compute overlap with multiple GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e4c10d-c1cd-4d62-a59c-31f8036ad520",
   "metadata": {},
   "source": [
    "### How to do copy/compute overlap with multiple GPU?\n",
    "\n",
    "Overlapping data transfers with computation across multiple GPUs is a powerful strategy in CUDA programming to maximize performance and hardware utilization. The goal is to minimize idle time by overlapping memory transfers (host-to-device and device-to-host) with kernel execution on multiple GPUs. This is achieved using asynchronous operations and streams. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373cbb64-a046-4766-a14c-d85f8265fe49",
   "metadata": {},
   "source": [
    "### Is there a step-by-step explanation of how to implement this technique?\n",
    "\n",
    "1. Prepare the Environment\n",
    "\n",
    "2. Divide the Data into Chunks\n",
    "\n",
    "3. Allocate Memory on Multiple GPUs\n",
    "\n",
    "4. Set Up CUDA Streams for Each GPU\n",
    "\n",
    "5. Copy Data to GPUs Asynchronously\n",
    "\n",
    "6. Launch Kernels Asynchronously\n",
    "\n",
    "7. Transfer Results Back to the Host Asynchronously\n",
    "\n",
    "8. Synchronize the Streams and Devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb5817-cfec-43e1-815d-b24cbc4b47f7",
   "metadata": {},
   "source": [
    "## `CUDA error handling`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79affc03-d2f6-4a46-b031-29643b833d3e",
   "metadata": {},
   "source": [
    "### What is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c7cfbd-4673-416c-9e16-b89816e2d068",
   "metadata": {},
   "source": [
    "CUDA error handling is a crucial aspect of CUDA programming that ensures the correct and stable execution of GPU-accelerated applications. It **involves detecting, reporting, and addressing errors that can occur during CUDA function calls**, kernel launches, memory operations, or any other GPU-related activity. Proper error handling helps developers identify issues, debug programs, and prevent undefined behavior or crashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dce084-fb8d-4c05-902b-48f1c898b6ae",
   "metadata": {},
   "source": [
    "### Why CUDA Error Handling is Important?\n",
    "\n",
    "- Detecting Failures\n",
    "\n",
    "- Debugging\n",
    "\n",
    "- Deploy the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9aa552-45ec-4a8f-b8eb-5f44ce908481",
   "metadata": {},
   "source": [
    "### Can you give me a sample example?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56a130-de1d-4859-8576-96ca471323e5",
   "metadata": {},
   "source": [
    "~~~c++\n",
    "myKernel<<<gridDim, blockDim>>>(args);\n",
    "\n",
    "cudaError_t err = cudaGetLastError();\n",
    "\n",
    "if (err != cudaSuccess) \n",
    "    printf(\"Kernel launch failed: %s\\n\", cudaGetErrorString(err));\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d1fc44-11c0-4372-9a7b-066ca59fffb2",
   "metadata": {},
   "source": [
    "## `NVIDIA® Nsight™ Systems Visual Profiler`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f006252a-36bf-4c59-9d3d-32307bd78872",
   "metadata": {},
   "source": [
    "### What is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb98d4b-1463-4e21-932f-3eb4936bd712",
   "metadata": {},
   "source": [
    "It is a **profilling tool**. The objective is to **identify bottenecks, and performance on GPU systems**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593b54f-43b1-446c-8bc2-b788fb503ddd",
   "metadata": {},
   "source": [
    "### What is possible with the tool?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65cc54-94e7-45cb-8971-b60ab8b94ac3",
   "metadata": {},
   "source": [
    "- Memory analysis\n",
    "- Identify latency\n",
    "- Debugging performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
