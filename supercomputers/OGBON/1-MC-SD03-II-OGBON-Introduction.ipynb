{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549339c3",
   "metadata": {},
   "source": [
    "# An Introduction the CUDAWARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1cc8f",
   "metadata": {},
   "source": [
    "Welcome to the webinar _An Introduction the CUDAWARE_ in Santos Dummont Summer School 2023 Program. In this webinar you will learn several techniques for scaling single GPU applications to multi-GPU and multiple nodes, with an emphasis on [NCCL (NVIDIA Collective Communications Library)](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html), [CUDAWARE-MPI](https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/), and [NVSHMEM](https://developer.nvidia.com/nvshmem) which allows for elegant multi-GPU application code and has been proven to scale very well on systems with many GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2465e",
   "metadata": {},
   "source": [
    "## The Coding Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b909c",
   "metadata": {},
   "source": [
    "The first step is display information about the CPU architecture with the command `lscpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8685bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea3b9c",
   "metadata": {},
   "source": [
    "In this node, we can observe that the multi-GPU resources connect with the NUMA nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eecce9",
   "metadata": {},
   "source": [
    "For your work today, you have access to several GPUs in the cloud. Run the following cell to see the GPUs available to you today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi topo -m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65989834",
   "metadata": {},
   "source": [
    "While your work today will be on a single node, all the techniques you learn today, in particular CUDAWARE-MPI and NVSHMEM, can be used to run your applications across clusters of multi-GPU nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfba876",
   "metadata": {},
   "source": [
    "Let us show the NVLink Status for different GPUs reported from `nvidia-smi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338366d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi nvlink --status -i 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666c4e4",
   "metadata": {},
   "source": [
    "In the end, it gives information about the NUMA memory nodes, with tue `lstopo` command, that is used to show the topology of the system.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f054097",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lstopo --of png > ogbon.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57470e88-3846-4cb2-9bb0-3edad6cbc2b1",
   "metadata": {},
   "source": [
    "This will import and display a .png image in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb812d0f-7679-4bec-8f0f-aca17b3d9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "path=\"ogbon.png\"\n",
    "display(Image.open(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804aec0",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56417956",
   "metadata": {},
   "source": [
    "During this short course today you will work through each of the following notebooks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d044ad",
   "metadata": {},
   "source": [
    "- [_NCCL_](2-MC-SD03-II-OGBON-NCCL-P2P.ipynb): In this notebook you will introduced the NCCL API, and the concepts of peer-to-peer communication between GPUs.\n",
    "\n",
    "- [_CUDAWARE-MPI_](3-MC-SD03-II-OGBON-CUDAWARE-MPI.ipynb): You will begin by familiarizing with the concepts of CUDAWARE-MPI API to multi-GPU nodes.\n",
    "\n",
    "- [_Monte Carlo Approximation of $\\pi$ - Single GPU_](4-MC-SD03-II-OGBON-MCπ-SGPU.ipynb): You will begin by familiarizing yourself with a single GPU implementation of the monte-carlo approximation of π algorithm, which we will use to introduce many multi GPU programming paradigms.\n",
    "\n",
    "- [_Monte Carlo Approximation of $\\pi$ - Multiple GPUs_](5-MC-SD03-II-OGBON-MCπ-MGPU.ipynb): In this notebook you will extend the monte-carlo π program to run on multiple GPUs by looping over available GPU devices.\n",
    "\n",
    "- [_Monte Carlo Approximation of $\\pi$ - CUDAWARE-MPI_](6-MC-SD03-II-OGBON-MCπ-CUDAWARE-MPI): In this notebook you will learn how to applied the CUDAWARE-MPI, and some concepts about peer-to-peer communication between GPUs in the SPMD paradigm.\n",
    "\n",
    "- [_Monte Carlo Approximation of $\\pi$ - NVSHMEM_](7-MC-SD03-II-OGBON-MCπ-NVSHMEM.ipynb): In this notebook you will be introduced to NVSHMEM, and will take your first pass with it using the monte-carlo π program.\n",
    "\n",
    "- [_Jacobi Iteration_](8-MC-SD03-I-OGBON-Jacobi.ipynb): In this notebook you will be introduced to a Laplace equation solver using Jacobi iteration and will learn how to use NVSHMEM to handle boundary communications between multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2542fdbe-de69-4868-b542-479d568f7cee",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f5799-e75b-4bcf-b93a-27b29f0b0b91",
   "metadata": {},
   "source": [
    "* NCCL (NVIDIA Collective Communications Library), https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html (accessed January 12, 2023).\n",
    "\n",
    "* CUDAWARE, https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/\n",
    "(accessed January 16, 2023).\n",
    "\n",
    "* NVSHMEM, https://developer.nvidia.com/nvshmem\n",
    "(accessed January 16, 2023).\n",
    "\n",
    "* CZARNUL, P. Parallel Programming for Modern High-Performance Computing Systems. New York: Chapman and Hall: CRC, 2018. 330 p.\n",
    "\n",
    "* DOWD, K.; SEVERANCE, C. An Introduction to Parallel Programming.\n",
    "Sebastopol: O’Reilly & Associates, 1998. 446 p.\n",
    "\n",
    "* HAGER, G.; WELLEIN, G. Introduction to High-Performance Computing for Scientists and Engineers. 2nd ed. Boca Raton: Chapman and Hall: CRC, 2018. 400 p.\n",
    "\n",
    "* HENNESSY, J. L.; PATTERSON, D. A. Computer Architecture: A Quantitative Approach. 6th ed. San Francisco: Elsevier: Morgan Kaufmann, 2017. 936 p.\n",
    "\n",
    "* PACHECO, P. S. An Introduction to Parallel Programming. San Francisco: Elsevier: Morgan Kaufmann, 2011. 392 p. \n",
    "\n",
    "* STERLING, T.; ANDERSON, M.; BRODOWICZ, M. High-Performance Computing: Modern Systems and Practices. Cambridge: Elsevier: Morgan Kaufmann, 2017. 718 p.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
