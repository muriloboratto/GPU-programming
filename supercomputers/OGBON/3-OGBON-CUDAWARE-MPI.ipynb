{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA-aware MPI on Multi-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will introduce how MPI and CUDA compatibility works, how efficient it is, and how it can be used on API CUDA-aware MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understand the concepts of MPI, CUDA and CUDA-aware MPI on multiple GPUs.\n",
    "- Understant the API CUDA-aware MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks Ping-Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will look at a simple *ping pong* code that measures the bandwidth for data transfers between 2 MPI processes. We will look at the following versions:\n",
    "\n",
    "- A first version using CPU with __MPI__;\n",
    "- A second version with __MPI + CUDA__ between two GPUs which processes data through CPU memory;\n",
    "- And the last one that uses __CUDA-aware MPI__ which exchange data directly between GPUs using GPUdirect or by NVLINK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by looking at a CPU-only version of the code to understand the idea behind a simple data transfer program (*ping-pong*). MPI processes pass data back and forth, and bandwidth is calculated by measuring the data transfers, as you know how much size is being transferred. Let is look at the `ping-pong-MPI.c` code to see how it is implemented. At the top of the main program, we start the MPI, determine the total number of processes and the rank identifiers, and make sure we only have two ranks in total to run the *ping-pong*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    int size, rank;\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Status status; \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then enter the main *loop* `for`, where each iteration performs data transfers and bandwidth calculations for different message size, ranging from 8 bytes to 1 GB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "   for(int i = 0; i <= 27; i++)\n",
    "     long int N = 1 << i; \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the *A* array, define some labels to match the MPI send/receive pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "   double *A = (double*) calloc (N, sizeof(double)); \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, each iteration of the *loop* does the following:\n",
    "\n",
    "- If rank is 0, it first sends a message with data from the matrix \\verb+A+ to rank 1, then expects to receive a message of rank 1.\n",
    "\n",
    "- If rank is 1, first expect to receive a message from rank 0 and then send a message back to rank 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    start_time = MPI_Wtime();\n",
    "    for(int i = 1; i <= loop_count; i++)\n",
    "    {\n",
    "      if(rank == 0)\n",
    "      {\n",
    "        MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "        MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &stat);\n",
    "      }else if(rank == 1)\n",
    "       {\n",
    "         MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &stat);\n",
    "         MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "       }\n",
    "    }\n",
    "    stop_time = MPI_Wtime();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous two points describe an application data transfer *ping-pong*. Now that we are familiar with the basic *ping-pong* code in MPI let us look at a version that includes GPUs with CUDA. In this example, we are still passing data back and forth between two MPI ratings, but the data is in GPU memory this time. More specifically, rank 0 has a memory buffer on GPU 0, and rank 1 has a memory buffer on GPU 1, and they will pass the data between the memories of the two GPUs. Here, to get data from memory from GPU 0 to GPU 1, we will first put the data into CPU memory *host*. Next, we can see the differences between the previous version to the new version with MPI+CUDA. Then, from the synchronization results and the known size of the data transfers, we calculate the bandwidth and print the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "long int num_B = 8 * N;\n",
    "long int B_in_GB = 1 << 30;\n",
    "double num_GB = (double)num_B / (double)B_in_GB;\n",
    "double avg_time_per_transfer=elapsed_time/(2.0*(double)loop_count);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in order to compile MPI programs, we must include the appropriate compilation option, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ping-pong-MPI.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile ping-pong-MPI.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <unistd.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    int size, rank;\n",
    "\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    MPI_Status status;\n",
    "\n",
    "    double start_time, stop_time, elapsed_time;\n",
    "       \n",
    "    for(int i = 0; i <= 27; i++) \n",
    "    {\n",
    "       long int N = 1 << i; /*Loop from 8 Bytes to 1 GB*/\n",
    "\n",
    "       double *A = (double*)calloc( N, sizeof(double));  /*Allocate memory for A on CPU*/\n",
    "\n",
    "       int tag1 = 1000;\n",
    "       int tag2 = 2000;\n",
    "\n",
    "       int loop_count = 50;\n",
    "\n",
    "       /********************************/      \n",
    "       /**/ start_time = MPI_Wtime();/**/\n",
    "       /********************************/\n",
    "\n",
    "       for(int i = 1; i <= loop_count; i++)\n",
    "       {\n",
    "            if(rank == 0)\n",
    "            {\n",
    "               MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "               MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &status);\n",
    "            }\n",
    "            else if(rank == 1)\n",
    "            {\n",
    "               MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n",
    "               MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "            }\n",
    "        }\n",
    "\n",
    "       /*********************************/      \n",
    "       /**/  stop_time = MPI_Wtime(); /**/\n",
    "       /********************************/      \n",
    "      \n",
    "        /*measured time*/\n",
    "        elapsed_time = stop_time - start_time;  \n",
    "        long int num_B = 8 * N;\n",
    "        long int B_in_GB = 1 << 30;\n",
    "        double num_GB = (double)num_B / (double)B_in_GB;\n",
    "        double avg_time_per_transfer = elapsed_time / (2.0*(double)loop_count);\n",
    "\n",
    "        if(rank == 0) \n",
    "            printf(\"Transfer size (Bytes): %10li, Transfer Time (seconds): %15.9f, Bandwidth (GB/s): %15.9f\\n\", \n",
    "                   num_B, avg_time_per_transfer, num_GB/avg_time_per_transfer );  \n",
    "\n",
    "        free(A);   \n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicxx ping-pong-MPI.c -o ping-pong-MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c018:61156] MCW rank 0 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]], socket 0[core 4[hwt 0-1]], socket 0[core 5[hwt 0-1]], socket 0[core 6[hwt 0-1]], socket 0[core 7[hwt 0-1]], socket 0[core 8[hwt 0-1]], socket 0[core 9[hwt 0-1]], socket 0[core 10[hwt 0-1]], socket 0[core 11[hwt 0-1]], socket 0[core 12[hwt 0-1]], socket 0[core 13[hwt 0-1]], socket 0[core 14[hwt 0-1]], socket 0[core 15[hwt 0-1]], socket 0[core 16[hwt 0-1]], socket 0[core 17[hwt 0-1]]: [BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB][../../../../../../../../../../../../../../../../../..]\n",
      "[c018:61156] MCW rank 1 bound to socket 1[core 18[hwt 0-1]], socket 1[core 19[hwt 0-1]], socket 1[core 20[hwt 0-1]], socket 1[core 21[hwt 0-1]], socket 1[core 22[hwt 0-1]], socket 1[core 23[hwt 0-1]], socket 1[core 24[hwt 0-1]], socket 1[core 25[hwt 0-1]], socket 1[core 26[hwt 0-1]], socket 1[core 27[hwt 0-1]], socket 1[core 28[hwt 0-1]], socket 1[core 29[hwt 0-1]], socket 1[core 30[hwt 0-1]], socket 1[core 31[hwt 0-1]], socket 1[core 32[hwt 0-1]], socket 1[core 33[hwt 0-1]], socket 1[core 34[hwt 0-1]], socket 1[core 35[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB]\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: There was an error initializing an OpenFabrics device.\n",
      "\n",
      "  Local host:   c018\n",
      "  Local device: mlx5_0\n",
      "--------------------------------------------------------------------------\n",
      "Transfer size (Bytes):          8, Transfer Time (seconds):     0.000026604, Bandwidth (GB/s):     0.000280058\n",
      "Transfer size (Bytes):         16, Transfer Time (seconds):     0.000000887, Bandwidth (GB/s):     0.016805568\n",
      "Transfer size (Bytes):         32, Transfer Time (seconds):     0.000001026, Bandwidth (GB/s):     0.029036627\n",
      "Transfer size (Bytes):         64, Transfer Time (seconds):     0.000001067, Bandwidth (GB/s):     0.055840964\n",
      "Transfer size (Bytes):        128, Transfer Time (seconds):     0.000002680, Bandwidth (GB/s):     0.044476597\n",
      "Transfer size (Bytes):        256, Transfer Time (seconds):     0.000001911, Bandwidth (GB/s):     0.124748758\n",
      "Transfer size (Bytes):        512, Transfer Time (seconds):     0.000001919, Bandwidth (GB/s):     0.248434204\n",
      "Transfer size (Bytes):       1024, Transfer Time (seconds):     0.000002267, Bandwidth (GB/s):     0.420613722\n",
      "Transfer size (Bytes):       2048, Transfer Time (seconds):     0.000003363, Bandwidth (GB/s):     0.567077641\n",
      "Transfer size (Bytes):       4096, Transfer Time (seconds):     0.000005320, Bandwidth (GB/s):     0.717095536\n",
      "Transfer size (Bytes):       8192, Transfer Time (seconds):     0.000005232, Bandwidth (GB/s):     1.458100560\n",
      "Transfer size (Bytes):      16384, Transfer Time (seconds):     0.000006876, Bandwidth (GB/s):     2.219253635\n",
      "Transfer size (Bytes):      32768, Transfer Time (seconds):     0.000011699, Bandwidth (GB/s):     2.608656617\n",
      "Transfer size (Bytes):      65536, Transfer Time (seconds):     0.000021376, Bandwidth (GB/s):     2.855278931\n",
      "Transfer size (Bytes):     131072, Transfer Time (seconds):     0.000040735, Bandwidth (GB/s):     2.996683269\n",
      "Transfer size (Bytes):     262144, Transfer Time (seconds):     0.000060162, Bandwidth (GB/s):     4.058087398\n",
      "Transfer size (Bytes):     524288, Transfer Time (seconds):     0.000109795, Bandwidth (GB/s):     4.447217345\n",
      "Transfer size (Bytes):    1048576, Transfer Time (seconds):     0.000212616, Bandwidth (GB/s):     4.593075121\n",
      "Transfer size (Bytes):    2097152, Transfer Time (seconds):     0.000409692, Bandwidth (GB/s):     4.767300685\n",
      "Transfer size (Bytes):    4194304, Transfer Time (seconds):     0.000808699, Bandwidth (GB/s):     4.830290039\n",
      "Transfer size (Bytes):    8388608, Transfer Time (seconds):     0.001593013, Bandwidth (GB/s):     4.904229362\n",
      "Transfer size (Bytes):   16777216, Transfer Time (seconds):     0.003164841, Bandwidth (GB/s):     4.937056277\n",
      "Transfer size (Bytes):   33554432, Transfer Time (seconds):     0.006293889, Bandwidth (GB/s):     4.965133814\n",
      "Transfer size (Bytes):   67108864, Transfer Time (seconds):     0.012575833, Bandwidth (GB/s):     4.969849525\n",
      "[c018:61156] 1 more process has sent help message help-mpi-btl-openib.txt / error in device init\n",
      "[c018:61156] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n",
      "Transfer size (Bytes):  134217728, Transfer Time (seconds):     0.025136017, Bandwidth (GB/s):     4.972943768\n",
      "Transfer size (Bytes):  268435456, Transfer Time (seconds):     0.050732194, Bandwidth (GB/s):     4.927837373\n",
      "Transfer size (Bytes):  536870912, Transfer Time (seconds):     0.101162406, Bandwidth (GB/s):     4.942547524\n",
      "Transfer size (Bytes): 1073741824, Transfer Time (seconds):     0.202618363, Bandwidth (GB/s):     4.935386832\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 2 ./ping-pong-MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise: Re-configuration of the processes using MPI\n",
    "\n",
    "- Assign the tags `--report-bindings`, and  `--map-by numa` in the compilation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun --report-bindings --map-by numa -np 2 ./ping-pong-MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain why the values of Bandwidth are differents using the tags?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MPI + CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with the basic *ping-pong* code in MPI let us look at a version that includes GPUs with CUDA. In this example, we are still passing data back and forth between two MPI ratings, but the data is in GPU memory this time. More specifically, rank 0 has a memory buffer on GPU 0, and rank 1 has a memory buffer on GPU 1, and they will pass the data between the memories of the two GPUs. Here, to get data from memory from GPU 0 to GPU 1, we will first put the data into CPU memory *host*. Next, we can see the differences between the previous version to the new version with MPI+CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    " start_time = MPI_Wtime();\n",
    " for(int i = 1; i <= loop_count; i++)\n",
    " {\n",
    "  if(rank == 0)\n",
    "  {\n",
    "   cudaMemcpy(A, d_A, N * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "   MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "   MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &status);\n",
    "   cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "   }else if(rank == 1)\n",
    "    {\n",
    "     MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n",
    "     cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "     cudaMemcpy(A, d_A, N * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "     MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "    }\n",
    " }\n",
    " stop_time = MPI_Wtime();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the CPU-only version, we initialize MPI and find the identifier of each MPI rank, but here we also assign each rank a different GPU (i.e., rank 0 is assigned to GPU 0 and rank 1 is mapped to GPU 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "int size, rank;\n",
    "MPI_Init(&argc, &argv);\n",
    "MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "MPI_Status status;\n",
    "\n",
    "cudaSetDevice(rank);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this release, each iteration of the \\emph{loop} does the following:\n",
    "\n",
    "- We enter the main *loop* `for`, which iterates over the different message sizes, and assign and initialize the __A__ array. However, we now have a call to `cudaMalloc` to reserve a memory buffer __d_A__ on the GPUs and a call to `cudaMemcpy` to transfer the data initialized in the *A* array to the buffer __d_A__. We need the command `cudaMemcpy` to get the data to the GPU before we start our *ping-pong*.\n",
    "\n",
    "- Data must first be transferred from GPU memory 0 to CPU memory. Then an MPI call is used to pass the data from ranks 0 to 1. Now that rank 1 has the data (in CPU memory), it can transfer it to GPU memory 1. Rank 0 must first transfer the data from a buffer in GPU 0 memory to one in CPU memory. Now that rank 1 contains the data in the CPU memory buffer, and it can transfer it to GPU 1 memory.\n",
    "\n",
    "As in the case where only the CPU is used, from the synchronization results and the known size of the data transfers, we calculate the bandwidth, print the results, and finally free up the memory of the computational resources. We ended the MPI and the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ping-pong-MPI+CUDA.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile ping-pong-MPI+CUDA.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda.h>\n",
    "#include <unistd.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    int size, rank;\n",
    "\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    MPI_Status status;\n",
    "\n",
    "    cudaSetDevice(rank);\n",
    "\n",
    "    double start_time, stop_time, elapsed_time;\n",
    "\n",
    "    for(int i = 0; i <= 27; i++)\n",
    "    {\n",
    "        long int N = 1 << i; /*Loop from 8 Bytes to 1 GB*/\n",
    "   \n",
    "        double *A = (double*)calloc(N, sizeof(double)); /*Allocate memory for A on CPU*/\n",
    "\n",
    "        double *d_A;\n",
    "\n",
    "        cudaMalloc(&d_A, N * sizeof(double)) ;\n",
    "        cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "\n",
    "        int tag1 = 1000;\n",
    "        int tag2 = 2000;\n",
    "\n",
    "        int loop_count = 50;\n",
    "\n",
    "       /********************************/      \n",
    "       /**/ start_time = MPI_Wtime();/**/\n",
    "       /********************************/\n",
    "\n",
    "        for(int i = 1; i <= loop_count; i++)\n",
    "        {\n",
    "            if(rank == 0)\n",
    "            {\n",
    "                cudaMemcpy(A, d_A, N * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "                MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "                MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &status);\n",
    "                cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "            }\n",
    "            else if(rank == 1)\n",
    "            {\n",
    "                MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n",
    "                cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "                cudaMemcpy(A, d_A, N * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "                MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "            }\n",
    "        }\n",
    "\n",
    "       /**********************************/      \n",
    "       /**/  stop_time = MPI_Wtime(); /**/\n",
    "       /*********************************/\n",
    "\n",
    "        /*measured time*/\n",
    "        elapsed_time = stop_time - start_time;\n",
    "        long int num_B = 8 * N;\n",
    "        long int B_in_GB = 1 << 30;\n",
    "        double num_GB = (double)num_B / (double)B_in_GB;\n",
    "        double avg_time_per_transfer = elapsed_time / (2.0*(double)loop_count);\n",
    "\n",
    "        if(rank == 0) \n",
    "          printf(\"Transfer size (Bytes): %10li, Transfer Time (seconds): %15.9f, Bandwidth (GB/s): %15.9f\\n\", \n",
    "                    num_B, avg_time_per_transfer, num_GB/avg_time_per_transfer );\n",
    "\n",
    "        cudaFree(d_A);\n",
    "        free(A);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing howtocompile.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh ogbon\"\n",
    "}\n",
    "\n",
    "ogbon()\n",
    "{\n",
    " nvcc -I/opt/share/openmpi/4.1.1-cuda/include -L/opt/share/openmpi/4.1.1-cuda/lib64 -lnccl -lmpi -o ping-pong-MPI+CUDA ping-pong-MPI+CUDA.cu\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#ogbon\n",
    "if [[ $1 == \"ogbon\" ]];then\n",
    " ogbon\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh ogbon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing howtoexecute.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh ogbon\"\n",
    "}\n",
    "\n",
    "ogbon()\n",
    "{\n",
    " sbatch slurm-MPI+CUDA.sh\n",
    "}\n",
    "\n",
    "localnode()\n",
    "{\n",
    " mpirun -np 2 --report-bindings --map-by numa -x UCX_MEMTYPE_CACHE=n  -mca pml ucx -mca btl ^vader,tcp,openib,smcuda -x UCX_NET_DEVICES=mlx5_0:1 ./ping-pong-MPI+CUDA\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#ogbon\n",
    "if [[ $1 == \"ogbon\" ]];then\n",
    " ogbon\n",
    "fi\n",
    "\n",
    "#localhost\n",
    "if [[ $1 == \"localnode\" ]];then\n",
    " localnode\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c018:45711] MCW rank 0 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]], socket 0[core 4[hwt 0-1]], socket 0[core 5[hwt 0-1]], socket 0[core 6[hwt 0-1]], socket 0[core 7[hwt 0-1]], socket 0[core 8[hwt 0-1]], socket 0[core 9[hwt 0-1]], socket 0[core 10[hwt 0-1]], socket 0[core 11[hwt 0-1]], socket 0[core 12[hwt 0-1]], socket 0[core 13[hwt 0-1]], socket 0[core 14[hwt 0-1]], socket 0[core 15[hwt 0-1]], socket 0[core 16[hwt 0-1]], socket 0[core 17[hwt 0-1]]: [BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB][../../../../../../../../../../../../../../../../../..]\n",
      "[c018:45711] MCW rank 1 bound to socket 1[core 18[hwt 0-1]], socket 1[core 19[hwt 0-1]], socket 1[core 20[hwt 0-1]], socket 1[core 21[hwt 0-1]], socket 1[core 22[hwt 0-1]], socket 1[core 23[hwt 0-1]], socket 1[core 24[hwt 0-1]], socket 1[core 25[hwt 0-1]], socket 1[core 26[hwt 0-1]], socket 1[core 27[hwt 0-1]], socket 1[core 28[hwt 0-1]], socket 1[core 29[hwt 0-1]], socket 1[core 30[hwt 0-1]], socket 1[core 31[hwt 0-1]], socket 1[core 32[hwt 0-1]], socket 1[core 33[hwt 0-1]], socket 1[core 34[hwt 0-1]], socket 1[core 35[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB]\n",
      "Transfer size (Bytes):          8, Transfer Time (seconds):     0.000047761, Bandwidth (GB/s):     0.000155998\n",
      "Transfer size (Bytes):         16, Transfer Time (seconds):     0.000013940, Bandwidth (GB/s):     0.001068941\n",
      "Transfer size (Bytes):         32, Transfer Time (seconds):     0.000013676, Bandwidth (GB/s):     0.002179130\n",
      "Transfer size (Bytes):         64, Transfer Time (seconds):     0.000013593, Bandwidth (GB/s):     0.004384922\n",
      "Transfer size (Bytes):        128, Transfer Time (seconds):     0.000015944, Bandwidth (GB/s):     0.007476895\n",
      "Transfer size (Bytes):        256, Transfer Time (seconds):     0.000014863, Bandwidth (GB/s):     0.016040660\n",
      "Transfer size (Bytes):        512, Transfer Time (seconds):     0.000015199, Bandwidth (GB/s):     0.031373343\n",
      "Transfer size (Bytes):       1024, Transfer Time (seconds):     0.000015885, Bandwidth (GB/s):     0.060035096\n",
      "Transfer size (Bytes):       2048, Transfer Time (seconds):     0.000017582, Bandwidth (GB/s):     0.108485868\n",
      "Transfer size (Bytes):       4096, Transfer Time (seconds):     0.000020171, Bandwidth (GB/s):     0.189113780\n",
      "Transfer size (Bytes):       8192, Transfer Time (seconds):     0.000022937, Bandwidth (GB/s):     0.332619846\n",
      "Transfer size (Bytes):      16384, Transfer Time (seconds):     0.000036998, Bandwidth (GB/s):     0.412417762\n",
      "Transfer size (Bytes):      32768, Transfer Time (seconds):     0.000051860, Bandwidth (GB/s):     0.588462636\n",
      "Transfer size (Bytes):      65536, Transfer Time (seconds):     0.000075514, Bandwidth (GB/s):     0.808260862\n",
      "Transfer size (Bytes):     131072, Transfer Time (seconds):     0.000126219, Bandwidth (GB/s):     0.967133720\n",
      "Transfer size (Bytes):     262144, Transfer Time (seconds):     0.000218215, Bandwidth (GB/s):     1.118807815\n",
      "Transfer size (Bytes):     524288, Transfer Time (seconds):     0.000391769, Bandwidth (GB/s):     1.246349985\n",
      "Transfer size (Bytes):    1048576, Transfer Time (seconds):     0.000733367, Bandwidth (GB/s):     1.331615820\n",
      "Transfer size (Bytes):    2097152, Transfer Time (seconds):     0.001249336, Bandwidth (GB/s):     1.563330466\n",
      "Transfer size (Bytes):    4194304, Transfer Time (seconds):     0.002354368, Bandwidth (GB/s):     1.659149986\n",
      "Transfer size (Bytes):    8388608, Transfer Time (seconds):     0.004539976, Bandwidth (GB/s):     1.720824098\n",
      "Transfer size (Bytes):   16777216, Transfer Time (seconds):     0.009137208, Bandwidth (GB/s):     1.710040955\n",
      "Transfer size (Bytes):   33554432, Transfer Time (seconds):     0.018920020, Bandwidth (GB/s):     1.651689589\n",
      "Transfer size (Bytes):   67108864, Transfer Time (seconds):     0.038081366, Bandwidth (GB/s):     1.641222654\n",
      "Transfer size (Bytes):  134217728, Transfer Time (seconds):     0.078161369, Bandwidth (GB/s):     1.599255509\n",
      "Transfer size (Bytes):  268435456, Transfer Time (seconds):     0.158132611, Bandwidth (GB/s):     1.580951572\n",
      "Transfer size (Bytes):  536870912, Transfer Time (seconds):     0.318826560, Bandwidth (GB/s):     1.568250776\n",
      "Transfer size (Bytes): 1073741824, Transfer Time (seconds):     0.637853691, Bandwidth (GB/s):     1.567757644\n"
     ]
    }
   ],
   "source": [
    "!bash howtoexecute.sh localnode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CUDA-aware MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at this code example, let us first describe [CUDA-aware MPI](https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/) and [GPUDirect RDMA](https://docs.nvidia.com/cuda/gpudirect-rdma/index.html). CUDA-aware MPI is an MPI implementation that allows GPU buffers (e.g., GPU memory allocated with cudaMalloc) to be used directly in MPI calls. However, CUDA-aware MPI alone does not specify whether data is stored in intermediate stages in CPU memory or passed from GPU to GPU. It will depend on the computational structure of the execution environment.\n",
    "\n",
    "The GPUDirect is an umbrella name used to refer to several specific technologies. In MPI, the GPUDirect technologies cover all kinds of inter-rank communication: intra-node, inter-node, and RDMA inter-node communication. Now let us take a look at the code below. It is the same as the tested version of MPI+CUDA, but now there are no calls to cudaMemcpy during the ping-pong steps. Instead, we use our GPU buffers (__d_A__) directly in MPI calls:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    start_time = MPI_Wtime();\n",
    "    for(int i = 1; i <= loop_count; i++)\n",
    "    {\n",
    "      if(rank == 0)\n",
    "      {\n",
    "        MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "        MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &stat);\n",
    "      }else if(rank == 1)\n",
    "       {\n",
    "         MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &stat);\n",
    "         MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "       }\n",
    "    }\n",
    "    stop_time = MPI_Wtime();\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ping-pong-CUDAWARE-MPI.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile ping-pong-CUDAWARE-MPI.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda.h>\n",
    "#include <unistd.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "\n",
    "    int size, rank;\n",
    "\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    MPI_Status status;\n",
    "\n",
    "    cudaSetDevice(rank);\n",
    "\n",
    "    double start_time, stop_time, elapsed_time;\n",
    "\n",
    "    for(int i = 0; i <= 27; i++)\n",
    "    {\n",
    "        long int N = 1 << i; /*Loop from 8 Bytes to 1 GB*/\n",
    "   \n",
    "        double *A = (double*)calloc(N, sizeof(double)); /*Allocate memory for A on CPU*/\n",
    "\n",
    "        double *d_A;\n",
    "\n",
    "        cudaMalloc(&d_A, N * sizeof(double)) ;\n",
    "        cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "\n",
    "        int tag1 = 1000;\n",
    "        int tag2 = 2000;\n",
    "\n",
    "        int loop_count = 50;\n",
    "\n",
    "       /********************************/      \n",
    "       /**/ start_time = MPI_Wtime();/**/\n",
    "       /********************************/\n",
    "\n",
    "        for(int i = 1; i <= loop_count; i++)\n",
    "        {\n",
    "            if(rank == 0)\n",
    "            {\n",
    "              MPI_Send(d_A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "              MPI_Recv(d_A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &status);\n",
    "            }\n",
    "            else if(rank == 1)\n",
    "            {\n",
    "              MPI_Recv(d_A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n",
    "              MPI_Send(d_A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "            }\n",
    "         }\n",
    "\n",
    "       /**********************************/      \n",
    "       /**/  stop_time = MPI_Wtime(); /**/\n",
    "       /*********************************/\n",
    "\n",
    "        /*measured time*/\n",
    "        elapsed_time = stop_time - start_time;\n",
    "        long int num_B = 8 * N;\n",
    "        long int B_in_GB = 1 << 30;\n",
    "        double num_GB = (double)num_B / (double)B_in_GB;\n",
    "        double avg_time_per_transfer = elapsed_time / (2.0*(double)loop_count);\n",
    "\n",
    "        if(rank == 0) \n",
    "            printf(\"Transfer size (Bytes): %10li, Transfer Time (seconds): %15.9f, Bandwidth (GB/s): %15.9f\\n\", \n",
    "                    num_B, avg_time_per_transfer, num_GB/avg_time_per_transfer );\n",
    "\n",
    "        cudaFree(d_A);\n",
    "        free(A);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting howtocompile.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh ogbon\"\n",
    "}\n",
    "\n",
    "ogbon()\n",
    "{\n",
    " nvcc -I/opt/share/openmpi/4.1.1-cuda/include -L/opt/share/openmpi/4.1.1-cuda/lib64 -lmpi ping-pong-CUDAWARE-MPI.cu -o ping-pong-CUDAWARE-MPI\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#ogbon\n",
    "if [[ $1 == \"ogbon\" ]];then\n",
    " ogbon\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh ogbon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting howtoexecute.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh ogbon\"\n",
    "}\n",
    "\n",
    "ogbon()\n",
    "{\n",
    " sbatch slurm-CUDAWARE-MPI.sh\n",
    "}\n",
    "\n",
    "localnode()\n",
    "{\n",
    " mpirun -np 2 --report-bindings --map-by numa -x UCX_MEMTYPE_CACHE=n -mca pml ucx -mca btl ^vader,tcp,openib,smcuda -x UCX_NET_DEVICES=mlx5_0:1 ./ping-pong-CUDAWARE-MPI\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#ogbon\n",
    "if [[ $1 == \"ogbon\" ]];then\n",
    " ogbon\n",
    "fi\n",
    "\n",
    "#localhost\n",
    "if [[ $1 == \"localnode\" ]];then\n",
    " localnode\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c018:64488] MCW rank 0 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]], socket 0[core 4[hwt 0-1]], socket 0[core 5[hwt 0-1]], socket 0[core 6[hwt 0-1]], socket 0[core 7[hwt 0-1]], socket 0[core 8[hwt 0-1]], socket 0[core 9[hwt 0-1]], socket 0[core 10[hwt 0-1]], socket 0[core 11[hwt 0-1]], socket 0[core 12[hwt 0-1]], socket 0[core 13[hwt 0-1]], socket 0[core 14[hwt 0-1]], socket 0[core 15[hwt 0-1]], socket 0[core 16[hwt 0-1]], socket 0[core 17[hwt 0-1]]: [BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB][../../../../../../../../../../../../../../../../../..]\n",
      "[c018:64488] MCW rank 1 bound to socket 1[core 18[hwt 0-1]], socket 1[core 19[hwt 0-1]], socket 1[core 20[hwt 0-1]], socket 1[core 21[hwt 0-1]], socket 1[core 22[hwt 0-1]], socket 1[core 23[hwt 0-1]], socket 1[core 24[hwt 0-1]], socket 1[core 25[hwt 0-1]], socket 1[core 26[hwt 0-1]], socket 1[core 27[hwt 0-1]], socket 1[core 28[hwt 0-1]], socket 1[core 29[hwt 0-1]], socket 1[core 30[hwt 0-1]], socket 1[core 31[hwt 0-1]], socket 1[core 32[hwt 0-1]], socket 1[core 33[hwt 0-1]], socket 1[core 34[hwt 0-1]], socket 1[core 35[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/BB]\n",
      "Transfer size (Bytes):          8, Transfer Time (seconds):     0.000009389, Bandwidth (GB/s):     0.000793553\n",
      "Transfer size (Bytes):         16, Transfer Time (seconds):     0.000005552, Bandwidth (GB/s):     0.002684154\n",
      "Transfer size (Bytes):         32, Transfer Time (seconds):     0.000004804, Bandwidth (GB/s):     0.006203260\n",
      "Transfer size (Bytes):         64, Transfer Time (seconds):     0.000006222, Bandwidth (GB/s):     0.009579260\n",
      "Transfer size (Bytes):        128, Transfer Time (seconds):     0.000006355, Bandwidth (GB/s):     0.018758287\n",
      "Transfer size (Bytes):        256, Transfer Time (seconds):     0.000006572, Bandwidth (GB/s):     0.036278706\n",
      "Transfer size (Bytes):        512, Transfer Time (seconds):     0.000008096, Bandwidth (GB/s):     0.058897725\n",
      "Transfer size (Bytes):       1024, Transfer Time (seconds):     0.000009606, Bandwidth (GB/s):     0.099278405\n",
      "Transfer size (Bytes):       2048, Transfer Time (seconds):     0.000013838, Bandwidth (GB/s):     0.137838409\n",
      "Transfer size (Bytes):       4096, Transfer Time (seconds):     0.000023675, Bandwidth (GB/s):     0.161125274\n",
      "Transfer size (Bytes):       8192, Transfer Time (seconds):     0.000035668, Bandwidth (GB/s):     0.213901581\n",
      "Transfer size (Bytes):      16384, Transfer Time (seconds):     0.000031982, Bandwidth (GB/s):     0.477098817\n",
      "Transfer size (Bytes):      32768, Transfer Time (seconds):     0.000027508, Bandwidth (GB/s):     1.109412217\n",
      "Transfer size (Bytes):      65536, Transfer Time (seconds):     0.000026946, Bandwidth (GB/s):     2.265098250\n",
      "Transfer size (Bytes):     131072, Transfer Time (seconds):     0.000028643, Bandwidth (GB/s):     4.261733090\n",
      "Transfer size (Bytes):     262144, Transfer Time (seconds):     0.000030955, Bandwidth (GB/s):     7.887075135\n",
      "Transfer size (Bytes):     524288, Transfer Time (seconds):     0.000036720, Bandwidth (GB/s):    13.297466739\n",
      "Transfer size (Bytes):    1048576, Transfer Time (seconds):     0.000048065, Bandwidth (GB/s):    20.317547204\n",
      "Transfer size (Bytes):    2097152, Transfer Time (seconds):     0.000070373, Bandwidth (GB/s):    27.753727508\n",
      "Transfer size (Bytes):    4194304, Transfer Time (seconds):     0.000107742, Bandwidth (GB/s):    36.255440636\n",
      "Transfer size (Bytes):    8388608, Transfer Time (seconds):     0.000194262, Bandwidth (GB/s):    40.216202304\n",
      "Transfer size (Bytes):   16777216, Transfer Time (seconds):     0.000368168, Bandwidth (GB/s):    42.439889770\n",
      "Transfer size (Bytes):   33554432, Transfer Time (seconds):     0.000714507, Bandwidth (GB/s):    43.736449836\n",
      "Transfer size (Bytes):   67108864, Transfer Time (seconds):     0.001451457, Bandwidth (GB/s):    43.060174286\n",
      "Transfer size (Bytes):  134217728, Transfer Time (seconds):     0.002882102, Bandwidth (GB/s):    43.371117615\n",
      "Transfer size (Bytes):  268435456, Transfer Time (seconds):     0.005739046, Bandwidth (GB/s):    43.561243675\n",
      "Transfer size (Bytes):  536870912, Transfer Time (seconds):     0.011460746, Bandwidth (GB/s):    43.627175861\n",
      "Transfer size (Bytes): 1073741824, Transfer Time (seconds):     0.022899325, Bandwidth (GB/s):    43.669409450\n"
     ]
    }
   ],
   "source": [
    "!bash howtoexecute.sh localnode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Comparison Performance internode: NCCL x CUDA-aware MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the following ping-pong code using NCCL within one compute node with the previous implementation of CUDA-aware MPI. The idea is to understand why the values differ since both pass through the same high-speed channel inside the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ping-pong-NCCL.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile ping-pong-NCCL.cu\n",
    "#include <iostream>\n",
    "#include <nccl.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <chrono>\n",
    "\n",
    "#define NUM_GPUS 2\n",
    "\n",
    "__global__ void print_values(int gpu_id, float *data) {\n",
    "  printf(\"GPU %d: %f\\n\", gpu_id, data[threadIdx.x]);\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "  ncclComm_t comms[NUM_GPUS];\n",
    "\n",
    "  cudaStream_t streams[NUM_GPUS];\n",
    "\n",
    "  // Initializing NCCL\n",
    "  ncclUniqueId id;\n",
    "  ncclGetUniqueId(&id);\n",
    "  ncclGroupStart();\n",
    "  for (int i = 0; i < NUM_GPUS; ++i) {\n",
    "    cudaSetDevice(i);\n",
    "    ncclCommInitRank(&comms[i], NUM_GPUS, id, i);\n",
    "  }\n",
    "  ncclGroupEnd();\n",
    "\n",
    "  // Create a stream on each GPU\n",
    "  for (int i = 0; i < NUM_GPUS; ++i) {\n",
    "    cudaSetDevice(i);\n",
    "    cudaStreamCreate(&streams[i]);\n",
    "  }\n",
    "\n",
    "  for (int i = 0; i <= 27; i++) {\n",
    "    long int N = 1 << i;\n",
    "    size_t numBytes = N * sizeof(float);\n",
    "    float *buffers[NUM_GPUS];\n",
    "\n",
    "    // Allocate memory on each GPU\n",
    "    for (int j = 0; j < NUM_GPUS; ++j) {\n",
    "      cudaSetDevice(j);\n",
    "      cudaMalloc(&buffers[j], numBytes);\n",
    "    }\n",
    "\n",
    "    // Initializing data on each GPU\n",
    "    for (int j = 0; j < NUM_GPUS; ++j) {\n",
    "      cudaSetDevice(j);\n",
    "      float *h_data = new float[N];\n",
    "      for (int k = 0; k < N; ++k) h_data[k] = j + 1.0f;\n",
    "      cudaMemcpy(buffers[j], h_data, numBytes, cudaMemcpyHostToDevice);\n",
    "      delete[] h_data;\n",
    "    }\n",
    "\n",
    "    int loop_count = 50;\n",
    "\n",
    "    // Performing ping-pong between GPUs and measuring time\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaSetDevice(0);\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    cudaEventRecord(start, streams[0]);\n",
    "\n",
    "    for (int j = 0; j < loop_count; ++j) {\n",
    "      int src = j % NUM_GPUS;\n",
    "      int dst = (j + 1) % NUM_GPUS;\n",
    "\n",
    "      ncclGroupStart();\n",
    "      cudaSetDevice(src);\n",
    "      ncclSend(buffers[src], N, ncclFloat, dst, comms[src], streams[src]);\n",
    "\n",
    "      cudaSetDevice(dst);\n",
    "      ncclRecv(buffers[dst], N, ncclFloat, src, comms[dst], streams[dst]);\n",
    "      ncclGroupEnd();\n",
    "    }\n",
    "\n",
    "    cudaEventRecord(stop, streams[0]);\n",
    "    cudaEventSynchronize(stop);\n",
    "\n",
    "    float elapsedTime;\n",
    "    cudaEventElapsedTime(&elapsedTime, start, stop);\n",
    "    \n",
    "    /*measured*/\n",
    "    long int num_B = 8 * N;\n",
    "    long int B_in_GB = 1 << 30;\n",
    "    double num_GB = (double)num_B / (double)B_in_GB;\n",
    "    double avg_time_per_transfer = (elapsedTime * 1e-3) / (2.0*(double)loop_count);\n",
    "    float bandwidth = num_GB/avg_time_per_transfer ;\n",
    "  \n",
    "    printf(\"Transfer size (Bytes): %10li, Transfer Time (seconds): %15.9f, Bandwidth (GB/s): %15.9f\\n\", \n",
    "                  num_B, avg_time_per_transfer, bandwidth  );\n",
    " \n",
    "    // Cleanup memory\n",
    "    for (int j = 0; j < NUM_GPUS; ++j) {\n",
    "      cudaSetDevice(j);\n",
    "      cudaFree(buffers[j]);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Destroy NCCL communicators\n",
    "  for (int i = 0; i < NUM_GPUS; ++i) \n",
    "  {\n",
    "    cudaSetDevice(i);\n",
    "    ncclCommDestroy(comms[i]);\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc ping-pong-NCCL.cu -o ping-pong-NCCL -lnccl -std=c++11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer size (Bytes):          8, Transfer Time (seconds):     0.000055060, Bandwidth (GB/s):     0.000135316\n",
      "Transfer size (Bytes):         16, Transfer Time (seconds):     0.000004209, Bandwidth (GB/s):     0.003540612\n",
      "Transfer size (Bytes):         32, Transfer Time (seconds):     0.000004209, Bandwidth (GB/s):     0.007081224\n",
      "Transfer size (Bytes):         64, Transfer Time (seconds):     0.000004198, Bandwidth (GB/s):     0.014196990\n",
      "Transfer size (Bytes):        128, Transfer Time (seconds):     0.000004157, Bandwidth (GB/s):     0.028673723\n",
      "Transfer size (Bytes):        256, Transfer Time (seconds):     0.000004229, Bandwidth (GB/s):     0.056375459\n",
      "Transfer size (Bytes):        512, Transfer Time (seconds):     0.000004168, Bandwidth (GB/s):     0.114413090\n",
      "Transfer size (Bytes):       1024, Transfer Time (seconds):     0.000004229, Bandwidth (GB/s):     0.225501835\n",
      "Transfer size (Bytes):       2048, Transfer Time (seconds):     0.000004168, Bandwidth (GB/s):     0.457652360\n",
      "Transfer size (Bytes):       4096, Transfer Time (seconds):     0.000004260, Bandwidth (GB/s):     0.895502508\n",
      "Transfer size (Bytes):       8192, Transfer Time (seconds):     0.000004424, Bandwidth (GB/s):     1.724671483\n",
      "Transfer size (Bytes):      16384, Transfer Time (seconds):     0.000005018, Bandwidth (GB/s):     3.041053295\n",
      "Transfer size (Bytes):      32768, Transfer Time (seconds):     0.000006318, Bandwidth (GB/s):     4.830198288\n",
      "Transfer size (Bytes):      65536, Transfer Time (seconds):     0.000006789, Bandwidth (GB/s):     8.990142822\n",
      "Transfer size (Bytes):     131072, Transfer Time (seconds):     0.000007270, Bandwidth (GB/s):    16.790040970\n",
      "Transfer size (Bytes):     262144, Transfer Time (seconds):     0.000008735, Bandwidth (GB/s):    27.950595856\n",
      "Transfer size (Bytes):     524288, Transfer Time (seconds):     0.000010158, Bandwidth (GB/s):    48.068264008\n",
      "Transfer size (Bytes):    1048576, Transfer Time (seconds):     0.000013230, Bandwidth (GB/s):    73.813804626\n",
      "Transfer size (Bytes):    2097152, Transfer Time (seconds):     0.000019405, Bandwidth (GB/s):   100.651641846\n",
      "Transfer size (Bytes):    4194304, Transfer Time (seconds):     0.000030720, Bandwidth (GB/s):   127.156578064\n",
      "Transfer size (Bytes):    8388608, Transfer Time (seconds):     0.000053576, Bandwidth (GB/s):   145.821762085\n",
      "Transfer size (Bytes):   16777216, Transfer Time (seconds):     0.000108636, Bandwidth (GB/s):   143.828720093\n",
      "Transfer size (Bytes):   33554432, Transfer Time (seconds):     0.000206203, Bandwidth (GB/s):   151.549774170\n",
      "Transfer size (Bytes):   67108864, Transfer Time (seconds):     0.000398080, Bandwidth (GB/s):   157.003616333\n",
      "Transfer size (Bytes):  134217728, Transfer Time (seconds):     0.000737905, Bandwidth (GB/s):   169.398574829\n",
      "Transfer size (Bytes):  268435456, Transfer Time (seconds):     0.001467976, Bandwidth (GB/s):   170.302551270\n",
      "Transfer size (Bytes):  536870912, Transfer Time (seconds):     0.002928210, Bandwidth (GB/s):   170.752792358\n",
      "Transfer size (Bytes): 1073741824, Transfer Time (seconds):     0.005849354, Bandwidth (GB/s):   170.959045410\n"
     ]
    }
   ],
   "source": [
    "!./ping-pong-NCCL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear the Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, please execute the following cell to clear up the CPU memory. This is required to move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please continue to the next notebook: [_4-OGBON-MC-SGPU.ipynb_](4-OGBON-MC-SGPU.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
