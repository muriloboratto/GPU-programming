{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDAWARE-MPI on Multi-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will introduce how MPI and CUDA compatibility works, how efficient it is, and how it can be used on API CUDAWARE-MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understand the concepts of MPI, CUDA and CUDAWARE-MPI on multiple GPUs.\n",
    "- Understant the API CUDAWARE-MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks Ping-Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will look at a simple *ping pong* code that measures the bandwidth for data transfers between 2 MPI processes. We will look at the following versions:\n",
    "\n",
    "- A first version using CPU with __MPI__;\n",
    "- A second version with __MPI + CUDA__ between two GPUs which processes data through CPU memory;\n",
    "- And the last one that uses __CUDAWARE-MPI__ which exchange data directly between GPUs using GPUdirect or by NVLINK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by looking at a CPU-only version of the code to understand the idea behind a simple data transfer program (*ping-pong*). MPI processes pass data back and forth, and bandwidth is calculated by measuring the data transfers, as you know how much size is being transferred. Let is look at the `ping-pong-MPI.c` code to see how it is implemented. At the top of the main program, we start the MPI, determine the total number of processes and the rank identifiers, and make sure we only have two ranks in total to run the *ping-pong*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    int size, rank;\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Status status; \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then enter the main *loop* `for`, where each iteration performs data transfers and bandwidth calculations for different message size, ranging from 8 bytes to 1 GB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "   for(int i = 0; i <= 27; i++)\n",
    "     long int N = 1 << i; \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the *A* array, define some labels to match the MPI send/receive pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "   double *A = (double*) calloc (N, sizeof(double)); \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, each iteration of the *loop* does the following:\n",
    "\n",
    "- If rank is 0, it first sends a message with data from the matrix \\verb+A+ to rank 1, then expects to receive a message of rank 1.\n",
    "\n",
    "- If rank is 1, first expect to receive a message from rank 0 and then send a message back to rank 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    start_time = MPI_Wtime();\n",
    "    for(int i = 1; i <= loop_count; i++)\n",
    "    {\n",
    "      if(rank == 0)\n",
    "      {\n",
    "        MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "        MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &stat);\n",
    "      }else if(rank == 1)\n",
    "       {\n",
    "         MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &stat);\n",
    "         MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "       }\n",
    "    }\n",
    "    stop_time = MPI_Wtime();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous two points describe an application data transfer *ping-pong*. Now that we are familiar with the basic *ping-pong* code in MPI let us look at a version that includes GPUs with CUDA. In this example, we are still passing data back and forth between two MPI ratings, but the data is in GPU memory this time. More specifically, rank 0 has a memory buffer on GPU 0, and rank 1 has a memory buffer on GPU 1, and they will pass the data between the memories of the two GPUs. Here, to get data from memory from GPU 0 to GPU 1, we will first put the data into CPU memory *host*. Next, we can see the differences between the previous version to the new version with MPI+CUDA. Then, from the synchronization results and the known size of the data transfers, we calculate the bandwidth and print the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "long int num_B = 8 * N;\n",
    "long int B_in_GB = 1 << 30;\n",
    "double num_GB = (double)num_B / (double)B_in_GB;\n",
    "double avg_time_per_transfer=elapsed_time/(2.0*(double)loop_count);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in order to compile MPI programs, we must include the appropriate compilation option, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ping-pong-MPI.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <unistd.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    int size, rank;\n",
    "\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    MPI_Status status;\n",
    "\n",
    "    double start_time, stop_time, elapsed_time;\n",
    "       \n",
    "    for(int i = 0; i <= 27; i++) \n",
    "    {\n",
    "       long int N = 1 << i; /*Loop from 8 Bytes to 1 GB*/\n",
    "\n",
    "       double *A = (double*)calloc( N, sizeof(double));  /*Allocate memory for A on CPU*/\n",
    "\n",
    "       int tag1 = 1000;\n",
    "       int tag2 = 2000;\n",
    "\n",
    "       int loop_count = 50;\n",
    "\n",
    "       /********************************/      \n",
    "       /**/ start_time = MPI_Wtime();/**/\n",
    "       /********************************/\n",
    "\n",
    "       for(int i = 1; i <= loop_count; i++)\n",
    "       {\n",
    "            if(rank == 0)\n",
    "            {\n",
    "               MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "               MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &status);\n",
    "            }\n",
    "            else if(rank == 1)\n",
    "            {\n",
    "               MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n",
    "               MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "            }\n",
    "        }\n",
    "\n",
    "       /*********************************/      \n",
    "       /**/  stop_time = MPI_Wtime(); /**/\n",
    "       /********************************/      \n",
    "      \n",
    "        /*measured time*/\n",
    "        elapsed_time = stop_time - start_time;  \n",
    "        long int num_B = 8 * N;\n",
    "        long int B_in_GB = 1 << 30;\n",
    "        double num_GB = (double)num_B / (double)B_in_GB;\n",
    "        double avg_time_per_transfer = elapsed_time / (2.0*(double)loop_count);\n",
    "\n",
    "        if(rank == 0) \n",
    "            printf(\"Transfer size (Bytes): %10li, Transfer Time (seconds): %15.9f, Bandwidth (GB/s): %15.9f\\n\", \n",
    "                   num_B, avg_time_per_transfer, num_GB/avg_time_per_transfer );  \n",
    "\n",
    "        free(A);   \n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpicc ping-pong-MPI.c -o ping-pong-MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 2 ./ping-pong-MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MPI + CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with the basic *ping-pong* code in MPI let us look at a version that includes GPUs with CUDA. In this example, we are still passing data back and forth between two MPI ratings, but the data is in GPU memory this time. More specifically, rank 0 has a memory buffer on GPU 0, and rank 1 has a memory buffer on GPU 1, and they will pass the data between the memories of the two GPUs. Here, to get data from memory from GPU 0 to GPU 1, we will first put the data into CPU memory *host*. Next, we can see the differences between the previous version to the new version with MPI+CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    " start_time = MPI_Wtime();\n",
    " for(int i = 1; i <= loop_count; i++)\n",
    " {\n",
    "  if(rank == 0)\n",
    "  {\n",
    "   cudaMemcpy(A, d_A, N * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "   MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "   MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &status);\n",
    "   cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "   }else if(rank == 1)\n",
    "    {\n",
    "     MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n",
    "     cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "     cudaMemcpy(A, d_A, N * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "     MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "    }\n",
    " }\n",
    " stop_time = MPI_Wtime();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the CPU-only version, we initialize MPI and find the identifier of each MPI rank, but here we also assign each rank a different GPU (i.e., rank 0 is assigned to GPU 0 and rank 1 is mapped to GPU 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "int size, rank;\n",
    "MPI_Init(&argc, &argv);\n",
    "MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "MPI_Status status;\n",
    "\n",
    "cudaSetDevice(rank);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this release, each iteration of the \\emph{loop} does the following:\n",
    "\n",
    "- We enter the main *loop* `for`, which iterates over the different message sizes, and assign and initialize the __A__ array. However, we now have a call to `cudaMalloc` to reserve a memory buffer __d_A__ on the GPUs and a call to `cudaMemcpy` to transfer the data initialized in the *A* array to the buffer __d_A__. We need the command `cudaMemcpy` to get the data to the GPU before we start our *ping-pong*.\n",
    "\n",
    "- Data must first be transferred from GPU memory 0 to CPU memory. Then an MPI call is used to pass the data from ranks 0 to 1. Now that rank 1 has the data (in CPU memory), it can transfer it to GPU memory 1. Rank 0 must first transfer the data from a buffer in GPU 0 memory to one in CPU memory. Now that rank 1 contains the data in the CPU memory buffer, and it can transfer it to GPU 1 memory.\n",
    "\n",
    "As in the case where only the CPU is used, from the synchronization results and the known size of the data transfers, we calculate the bandwidth, print the results, and finally free up the memory of the computational resources. We ended the MPI and the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ping-pong-MPI+CUDA.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda.h>\n",
    "#include <unistd.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    int size, rank;\n",
    "\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    MPI_Status status;\n",
    "\n",
    "    cudaSetDevice(rank);\n",
    "\n",
    "    double start_time, stop_time, elapsed_time;\n",
    "\n",
    "    for(int i = 0; i <= 27; i++)\n",
    "    {\n",
    "        long int N = 1 << i; /*Loop from 8 Bytes to 1 GB*/\n",
    "   \n",
    "        double *A = (double*)calloc(N, sizeof(double)); /*Allocate memory for A on CPU*/\n",
    "\n",
    "        double *d_A;\n",
    "\n",
    "        cudaMalloc(&d_A, N * sizeof(double)) ;\n",
    "        cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "\n",
    "        int tag1 = 1000;\n",
    "        int tag2 = 2000;\n",
    "\n",
    "        int loop_count = 50;\n",
    "\n",
    "       /********************************/      \n",
    "       /**/ start_time = MPI_Wtime();/**/\n",
    "       /********************************/\n",
    "\n",
    "        for(int i = 1; i <= loop_count; i++)\n",
    "        {\n",
    "            if(rank == 0)\n",
    "            {\n",
    "                cudaMemcpy(A, d_A, N * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "                MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "                MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &status);\n",
    "                cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "            }\n",
    "            else if(rank == 1)\n",
    "            {\n",
    "                MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n",
    "                cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "                cudaMemcpy(A, d_A, N * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "                MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "            }\n",
    "        }\n",
    "\n",
    "       /**********************************/      \n",
    "       /**/  stop_time = MPI_Wtime(); /**/\n",
    "       /*********************************/\n",
    "\n",
    "        /*measured time*/\n",
    "        elapsed_time = stop_time - start_time;\n",
    "        long int num_B = 8 * N;\n",
    "        long int B_in_GB = 1 << 30;\n",
    "        double num_GB = (double)num_B / (double)B_in_GB;\n",
    "        double avg_time_per_transfer = elapsed_time / (2.0*(double)loop_count);\n",
    "\n",
    "        if(rank == 0) \n",
    "          printf(\"Transfer size (Bytes): %10li, Transfer Time (seconds): %15.9f, Bandwidth (GB/s): %15.9f\\n\", \n",
    "                    num_B, avg_time_per_transfer, num_GB/avg_time_per_transfer );\n",
    "\n",
    "        cudaFree(d_A);\n",
    "        free(A);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh ogbon\"\n",
    "}\n",
    "\n",
    "ogbon()\n",
    "{\n",
    " nvcc -I/opt/share/openmpi/4.1.1-cuda/include -L/opt/share/openmpi/4.1.1-cuda/lib64 -lnccl -lmpi -o ping-pong-MPI+CUDA ping-pong-MPI+CUDA.cu\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#ogbon\n",
    "if [[ $1 == \"ogbon\" ]];then\n",
    " ogbon\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh ogbon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh ogbon\"\n",
    "}\n",
    "\n",
    "ogbon()\n",
    "{\n",
    " sbatch slurm-MPI+CUDA.sh\n",
    "}\n",
    "\n",
    "localnode()\n",
    "{\n",
    " mpirun -np 2 --report-bindings --map-by numa -x UCX_MEMTYPE_CACHE=n  -mca pml ucx -mca btl ^vader,tcp,openib,smcuda -x UCX_NET_DEVICES=mlx5_0:1 ./ping-pong-MPI+CUDA\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#ogbon\n",
    "if [[ $1 == \"ogbon\" ]];then\n",
    " ogbon\n",
    "fi\n",
    "\n",
    "#localhost\n",
    "if [[ $1 == \"localnode\" ]];then\n",
    " localnode\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtoexecute.sh localnode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CUDAWARE-MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at this code example, let us first describe [CUDAWARE-MPI](https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/) and [GPUDirect RDMA](https://docs.nvidia.com/cuda/gpudirect-rdma/index.html). CUDAWARE-MPI is an MPI implementation that allows GPU buffers (e.g., GPU memory allocated with cudaMalloc) to be used directly in MPI calls. However, CUDAWARE-MPI alone does not specify whether data is stored in intermediate stages in CPU memory or passed from GPU to GPU. It will depend on the computational structure of the execution environment.\n",
    "\n",
    "The GPUDirect is an umbrella name used to refer to several specific technologies. In MPI, the GPUDirect technologies cover all kinds of inter-rank communication: intra-node, inter-node, and RDMA inter-node communication. Now let us take a look at the code below. It is the same as the tested version of MPI+CUDA, but now there are no calls to cudaMemcpy during the ping-pong steps. Instead, we use our GPU buffers (__d_A__) directly in MPI calls:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    start_time = MPI_Wtime();\n",
    "    for(int i = 1; i <= loop_count; i++)\n",
    "    {\n",
    "      if(rank == 0)\n",
    "      {\n",
    "        MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "        MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &stat);\n",
    "      }else if(rank == 1)\n",
    "       {\n",
    "         MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &stat);\n",
    "         MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "       }\n",
    "    }\n",
    "    stop_time = MPI_Wtime();\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ping-pong-CUDAWARE-MPI.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda.h>\n",
    "#include <unistd.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "\n",
    "    int size, rank;\n",
    "\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    MPI_Status status;\n",
    "\n",
    "    cudaSetDevice(rank);\n",
    "\n",
    "    double start_time, stop_time, elapsed_time;\n",
    "\n",
    "    for(int i = 0; i <= 27; i++)\n",
    "    {\n",
    "        long int N = 1 << i; /*Loop from 8 Bytes to 1 GB*/\n",
    "   \n",
    "        double *A = (double*)calloc(N, sizeof(double)); /*Allocate memory for A on CPU*/\n",
    "\n",
    "        double *d_A;\n",
    "\n",
    "        cudaMalloc(&d_A, N * sizeof(double)) ;\n",
    "        cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "\n",
    "        int tag1 = 1000;\n",
    "        int tag2 = 2000;\n",
    "\n",
    "        int loop_count = 50;\n",
    "\n",
    "       /********************************/      \n",
    "       /**/ start_time = MPI_Wtime();/**/\n",
    "       /********************************/\n",
    "\n",
    "        for(int i = 1; i <= loop_count; i++)\n",
    "        {\n",
    "            if(rank == 0)\n",
    "            {\n",
    "              MPI_Send(d_A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "              MPI_Recv(d_A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &status);\n",
    "            }\n",
    "            else if(rank == 1)\n",
    "            {\n",
    "              MPI_Recv(d_A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n",
    "              MPI_Send(d_A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "            }\n",
    "         }\n",
    "\n",
    "       /**********************************/      \n",
    "       /**/  stop_time = MPI_Wtime(); /**/\n",
    "       /*********************************/\n",
    "\n",
    "        /*measured time*/\n",
    "        elapsed_time = stop_time - start_time;\n",
    "        long int num_B = 8 * N;\n",
    "        long int B_in_GB = 1 << 30;\n",
    "        double num_GB = (double)num_B / (double)B_in_GB;\n",
    "        double avg_time_per_transfer = elapsed_time / (2.0*(double)loop_count);\n",
    "\n",
    "        if(rank == 0) \n",
    "            printf(\"Transfer size (Bytes): %10li, Transfer Time (seconds): %15.9f, Bandwidth (GB/s): %15.9f\\n\", \n",
    "                    num_B, avg_time_per_transfer, num_GB/avg_time_per_transfer );\n",
    "\n",
    "        cudaFree(d_A);\n",
    "        free(A);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh ogbon\"\n",
    "}\n",
    "\n",
    "ogbon()\n",
    "{\n",
    " nvcc -I/opt/share/openmpi/4.1.1-cuda/include -L/opt/share/openmpi/4.1.1-cuda/lib64 -lmpi ping-pong-CUDAWARE-MPI.cu -o ping-pong-CUDAWARE-MPI\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#ogbon\n",
    "if [[ $1 == \"ogbon\" ]];then\n",
    " ogbon\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh ogbon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh ogbon\"\n",
    "}\n",
    "\n",
    "ogbon()\n",
    "{\n",
    " sbatch slurm-CUDAWARE-MPI.sh\n",
    "}\n",
    "\n",
    "localnode()\n",
    "{\n",
    " mpirun -np 2 --report-bindings --map-by numa -x UCX_MEMTYPE_CACHE=n -mca pml ucx -mca btl ^vader,tcp,openib,smcuda -x UCX_NET_DEVICES=mlx5_0:1 ./ping-pong-CUDAWARE-MPI\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#ogbon\n",
    "if [[ $1 == \"ogbon\" ]];then\n",
    " ogbon\n",
    "fi\n",
    "\n",
    "#localhost\n",
    "if [[ $1 == \"localnode\" ]];then\n",
    " localnode\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtoexecute.sh localnode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Matrix Multiply in CUDAWARE-MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Program a matrix multiplication using the CUDAWARE-MPI where matrices are generated in process 0, and all processes work in parts of the multiplication. The final result will finally be compiled in process 0. Make the implementation with distribution by blocks of lines and compare the execution times varying the size of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OpenMPI, https://www.open-mpi.org/doc/current/man1/mpirun.1.php\n",
    "(accessed January 12, 2023).\n",
    "\n",
    "* CUDAWARE, https://github.com/olcf-tutorials/MPI_ping_pong\n",
    "(accessed January 16, 2023).\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
