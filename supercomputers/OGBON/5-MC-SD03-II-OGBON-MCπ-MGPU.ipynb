{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Case Study: Monte Carlo Approximation of $\\pi$ - Multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will refactor the single GPU implementation of the monte carlo approximation of $\\pi$ algorithm to run on multiple GPUs using a technique of looping over available GPU devices to perform work on each. While this is a perfectly valid technique, we hope to begin demonstrating that it can quickly add significant complexity to your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Be able to utilize multiple GPUs by looping over them to perform work on each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending to Multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to extend our example to multiple GPUs is to use a single host process that manages multiple GPUs. If we have *M* GPUs and *N* sample points to evaluate, we can distribute *N/M* to each GPU, and in principle calculate the result up to *M* times more quickly.\n",
    "\n",
    "To enact this approach, we:\n",
    "- Use `cudaGetDeviceCount` to ascertain the number of available GPUs.\n",
    "- Loop over the number of GPUs, using `cudaSetDevice` in each loop iteration.\n",
    "- Perform the correct fraction of the work for the set GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "int device_count;\n",
    "cudaGetDeviceCount(&device_count);\n",
    "\n",
    "for (int i = 0; i < device_count; ++i) \n",
    "{\n",
    "    cudaSetDevice(i);\n",
    "    # Do single GPU worth of work.\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor to Multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this example we are giving each GPU a different seed for the random number generator so that each GPU is doing different work. As a result our answer will change a little. You can consult the code here [Monte Carlo Multi-GPU](solutions/monte_carlo_mgpu_cuda.cpp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile monte_carlo_mgpu_cuda.cu\n",
    "#include <iostream>\n",
    "#include <curand_kernel.h>\n",
    "#define N 1024*1024\n",
    "\n",
    "__global__ void calculate_pi(int* hits, int device) \n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    // Initialize random number state (unique for every thread in the grid)\n",
    "    int seed = device;\n",
    "    int offset = 0;\n",
    "    curandState_t curand_state;\n",
    "    curand_init(seed, idx, offset, &curand_state);\n",
    "\n",
    "    // Generate random coordinates within (0.0, 1.0]\n",
    "    float x = curand_uniform(&curand_state);\n",
    "    float y = curand_uniform(&curand_state);\n",
    "\n",
    "    // Increment hits counter if this point is inside the circle\n",
    "    if (x * x + y * y <= 1.0f) \n",
    "        atomicAdd(hits, 1);\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "int main(int argc, char** argv) \n",
    "{\n",
    "    // Determine number of GPUs\n",
    "    int device_count;\n",
    "    cudaGetDeviceCount(&device_count);\n",
    "\n",
    "    std::cout << \"Using \" << device_count << \" GPUs\" << std::endl;\n",
    "\n",
    "    // Allocate host and device values (one per GPU)\n",
    "    int** hits = (int**) malloc(device_count * sizeof(int*));\n",
    "    for (int i = 0; i < device_count; ++i) \n",
    "        hits[i] = (int*) malloc(sizeof(int));\n",
    "    \n",
    "\n",
    "    int** d_hits = (int**) malloc(device_count * sizeof(int*));\n",
    "    for (int i = 0; i < device_count; ++i) \n",
    "    {\n",
    "        cudaSetDevice(i);\n",
    "        cudaMalloc((void**) &d_hits[i], sizeof(int));\n",
    "    }\n",
    "\n",
    "    // Initialize number of hits and copy to device\n",
    "    for (int i = 0; i < device_count; ++i) \n",
    "    {\n",
    "        *hits[i] = 0;\n",
    "        cudaSetDevice(i);\n",
    "        cudaMemcpy(d_hits[i], hits[i], sizeof(int), cudaMemcpyHostToDevice);\n",
    "    }\n",
    "\n",
    "    // Launch kernel to do the calculation\n",
    "    int threads_per_block = 256;\n",
    "    int blocks = (N / device_count + threads_per_block - 1) / threads_per_block;\n",
    "\n",
    "    // Allow for asynchronous execution by launching all kernels first\n",
    "    // and then synchronizing on all devices after.\n",
    "    for (int i = 0; i < device_count; ++i) \n",
    "    {\n",
    "        cudaSetDevice(i);\n",
    "        calculate_pi<<<blocks, threads_per_block>>>(d_hits[i], i);\n",
    "    }\n",
    "\n",
    "    for (int i = 0; i < device_count; ++i) \n",
    "    {\n",
    "        cudaSetDevice(i);\n",
    "        cudaDeviceSynchronize();\n",
    "    }\n",
    "\n",
    "    // Copy final result back to the host\n",
    "    for (int i = 0; i < device_count; ++i) \n",
    "    {\n",
    "        cudaSetDevice(i);\n",
    "        cudaMemcpy(hits[i], d_hits[i], sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    }\n",
    "\n",
    "    // Sum number of hits over all devices\n",
    "    int hits_total = 0;\n",
    "    for (int i = 0; i < device_count; ++i) \n",
    "        hits_total += *hits[i];\n",
    "\n",
    "    // Calculate final value of pi\n",
    "    float pi_est = (float) hits_total / (float) (N) * 4.0f;\n",
    "\n",
    "    // Print out result\n",
    "    std::cout << \"Estimated value of pi = \" << pi_est << std::endl;\n",
    "    std::cout << \"Error = \" << std::abs((M_PI - pi_est) / pi_est) << std::endl;\n",
    "\n",
    "    // Clean up\n",
    "    for (int i = 0; i < device_count; ++i) \n",
    "    {\n",
    "        free(hits[i]);\n",
    "        cudaFree(d_hits[i]);\n",
    "    }\n",
    "    free(hits);\n",
    "    free(d_hits);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc monte_carlo_mgpu_cuda.cu -o monte_carlo_mgpu_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!./monte_carlo_mgpu_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NVIDIA DEVELOPER, https://developer.nvidia.com\n",
    "(accessed January 12, 2023)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
