{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Case Study: Jacobi Iteration - NVSHMEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook and the next we will look at a Laplace equation solver using Jacobi iteration. This problem will give us excellent opportunity to explore the common motif of handling data communication at boundary points between distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Be able to use NVSHMEM to handle boundary point communications in multi GPU algorithms with distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By no means do you need to fully understand this algorithm to be able to accomplish the objectives of the course. However, for the curious, and for context, we will start by spending some time discussing the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Laplace Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common motif in finite-element/finite-volume/finite-difference applications is the solution of elliptic partial differential equations with relaxation methods. Perhaps the simplest elliptic PDE is the Laplace equation:\n",
    "\n",
    "$$\n",
    "\\nabla^2\\, f = 0\n",
    "$$\n",
    "\n",
    "where $\\nabla^2 = \\nabla \\cdot \\nabla$ is the Laplacian operator (sum of second derivatives for all coordinate directions) and $f = f(\\mathbf{r})$ is a scalar field as a function of the spatial vector coordinate $\\mathbf{r}$. The Laplace equation can be used to solve, for example, the equilibrium distribution of temperature on a metal plate that is heated to a fixed temperature on its edges.\n",
    "\n",
    "In one dimension, where $f = f(x)$, this equation is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial x^2} f = 0\n",
    "$$\n",
    "\n",
    "Suppose we want to solve this equation over a domain $x = [0, L]$, given fixed boundary conditions $f(0) = T_{\\rm left}$ and $f(L) = T_{\\rm right}$. That is, we want to know what the temperature distribution looks like in the interior of the domain as a function of $x$. A common approach is to discretize space into a set of $N$ points, located at $0, L\\, /\\, (N - 1),\\, 2L\\,/\\,(N - 1),\\, ...,\\, (N - 2)\\,L\\, /\\, (N - 1),\\, L$. The leftmost and rightmost points will remain at the fixed temperatures $T_{\\rm left}$ and $T_{\\rm right}$ respectively, while the interior $N-2$ points are the unknowns we need to solve for. The distance between the points is $\\Delta x = L\\, /\\, (N - 1)$, and we will store the points in an array of length $N$. For each index $i$ in the (zero-indexed) array, the coordinate position is $i\\, L\\, /\\, (N - 1) = i\\, \\Delta x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a discretized spatial domain, the derivatives of the field at index $i$ are some function of the nearby points. For example, a simple discretization of the first derivative would be:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} f_i = (f_{i+1} - f_{i-1})\\ /\\ (2\\, \\Delta x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/1D_finite_differencing.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a simple discretization of the second derivative would be:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial x^2} f_i = (f_{i+1} - 2\\, f_{i} + f_{i-1})\\ /\\ (\\Delta x^2)\n",
    "$$\n",
    "\n",
    "If we set this expression equal to zero to satisfy the Laplace equation, we get:\n",
    "\n",
    "$$\n",
    "f_{i+1} - 2\\, f_{i} + f_{i-1} = 0\n",
    "$$\n",
    "\n",
    "Solving this for $f_{i}$, we get:\n",
    "\n",
    "$$\n",
    "f_{i} = (f_{i+1} + f_{i-1})\\ / \\ 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobi Iteration to Solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although $f_{i+1}$ and $f_{i-1}$ are also varying (except at the boundary points $i == 0$ and $i == N-1$), it turns out that we can simply *iterate* on this solution for $f_{i}$ many times until the solution is sufficiently equilibrated. That is, if in every iteration we take the old solution to $f$, and then at every point in the new solution set it equal to the average of the two neighboring points from the old solution, we will eventually solve for the equilibrium distribution of $f$.\n",
    "\n",
    "Depicting this approach in (serial) pseudo-code we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "while (error > tolerance):\n",
    "    l2_norm = 0\n",
    "    for i = 1, N-2:\n",
    "        f[i] = 0.5 * (f_old[i-1] + f_old[i+1])\n",
    "        l2_norm += (f[i] - f_old[i]) * (f[i] - f_old[i])\n",
    "    error = sqrt(l2_norm / N)\n",
    "    swap(f_old, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Single GPU CUDA Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is implemented in standard CUDA for a single GPU in the code `jacobi.cu`. Take some time to review the algorithm and its parallel implementation. As before, we are not aiming for the highest-performing solution, just something that sketches out the basic idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing jacobi.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile jacobi.cu\n",
    "#include <iostream>\n",
    "#include <limits>\n",
    "#include <cstdio>\n",
    "#define NUM_POINTS 4194304\n",
    "#define TOLERANCE  0.0001\n",
    "#define MAX_ITERS  1000\n",
    "\n",
    "__global__ void jacobi (const float* f_old, float* f, float* l2_norm, int N)\n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    if (idx > 0 && idx < N - 1) \n",
    "    {\n",
    "        f[idx] = 0.5f * (f_old[idx+1] + f_old[idx-1]);\n",
    "\n",
    "        float l2 = (f[idx] - f_old[idx]) * (f[idx] - f_old[idx]);\n",
    "\n",
    "        atomicAdd(l2_norm, l2);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void initialize (float* f, float T_left, float T_right, int N)\n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    \n",
    "    if (idx == 0) \n",
    "    {\n",
    "        f[idx] = T_left;\n",
    "    }\n",
    "    else if (idx == N - 1) \n",
    "    {\n",
    "        f[idx] = T_right;\n",
    "    }\n",
    "    else if (idx < N - 1) \n",
    "    {\n",
    "        f[idx] = 0.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    // Allocate space for the grid data, and the temporary buffer\n",
    "    // for the \"old\" data.\n",
    "    float* f_old;\n",
    "    float* f;\n",
    "\n",
    "    cudaMalloc(&f_old, NUM_POINTS * sizeof(float));\n",
    "    cudaMalloc(&f, NUM_POINTS * sizeof(float));\n",
    "\n",
    "    // Allocate memory for the L2 norm, on both the host and device.\n",
    "    float* l2_norm = (float*) malloc(sizeof(float));\n",
    "    float* d_l2_norm;\n",
    "    cudaMalloc(&d_l2_norm, sizeof(float));\n",
    "\n",
    "    // Initialize the error to a large number.\n",
    "    float error = std::numeric_limits<float>::max();\n",
    "\n",
    "    // Initialize the data.\n",
    "    int threads_per_block = 256;\n",
    "    int blocks = (NUM_POINTS + threads_per_block - 1) / threads_per_block;\n",
    "\n",
    "    float T_left = 5.0f;\n",
    "    float T_right = 10.0f;\n",
    "    initialize<<<blocks, threads_per_block>>>(f_old, T_left, T_right, NUM_POINTS);\n",
    "    initialize<<<blocks, threads_per_block>>>(f, T_left, T_right, NUM_POINTS);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Now iterate until the error is sufficiently small.\n",
    "    // As a safety mechanism, cap the number of iterations.\n",
    "    int num_iters = 0;\n",
    "\n",
    "    while (error > TOLERANCE && num_iters < MAX_ITERS) \n",
    "    {\n",
    "        // Initialize the norm data to zero\n",
    "        cudaMemset(d_l2_norm, 0, sizeof(float));\n",
    "\n",
    "        // Launch kernel to do the calculation\n",
    "        jacobi<<<blocks, threads_per_block>>>(f_old, f, d_l2_norm, NUM_POINTS);\n",
    "        cudaDeviceSynchronize();\n",
    "\n",
    "        // Swap f_old and f\n",
    "        std::swap(f_old, f);\n",
    "\n",
    "        // Update norm on host; calculate error by normalizing by number of zones and take square root\n",
    "        cudaMemcpy(l2_norm, d_l2_norm, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "        if (*l2_norm == 0.0f) \n",
    "        {\n",
    "            // Deal with first iteration\n",
    "            error = 1.0f;\n",
    "        }\n",
    "        else \n",
    "        {\n",
    "            error = std::sqrt(*l2_norm / NUM_POINTS);\n",
    "        }\n",
    "\n",
    "        if (num_iters % 10 == 0) \n",
    "        {\n",
    "            std::cout << \"Iteration = \" << num_iters << \" error = \" << error << std::endl;\n",
    "        }\n",
    "\n",
    "        ++num_iters;\n",
    "    }\n",
    "\n",
    "    if (error <= TOLERANCE && num_iters < MAX_ITERS) \n",
    "    {\n",
    "        std::cout << \"Success!\\n\";\n",
    "    }\n",
    "    else \n",
    "    {\n",
    "        std::cout << \"Failure!\\n\";\n",
    "    }\n",
    "\n",
    "    // Clean up\n",
    "    free(l2_norm);\n",
    "    cudaFree(d_l2_norm);\n",
    "    cudaFree(f_old);\n",
    "    cudaFree(f);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 0 error = 0.00272958\n",
      "Iteration = 10 error = 0.00034546\n",
      "Iteration = 20 error = 0.000210903\n",
      "Iteration = 30 error = 0.000157015\n",
      "Iteration = 40 error = 0.000127122\n",
      "Iteration = 50 error = 0.00010783\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "!nvcc -x cu -arch=sm_70 jacobi.cu -o jacobi \n",
    "!./jacobi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributing with NVSHMEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple distribution strategy to multiple GPUs is to divide the domain into $M$ chunks (where $M$ is the number of GPUs). PE 0 will have points $[0, N\\, /\\, M - 1]$, PE 1 will have points $[N\\, /\\, M,\\, 2\\, N\\, /\\, M - 1]$, etc. In this approach, the communication between PEs needs to happen at the boundary points between PEs. For example, the update at point $i = N\\, /\\, M - 1$ on PE 0 is:\n",
    "\n",
    "$f[N\\, /\\, M - 1] = (f[N\\, /\\, M] + f[N\\, /\\, M-2])\\ /\\ 2$\n",
    "\n",
    "But this PE doesn't own the data point at $i = N\\, /\\, M$, it is owned by PE 1. So we will need to get that data point from the remote PE. To do so, we can use the [nvshmem_float_g()](https://docs.nvidia.com/hpc-sdk/nvshmem/api/gen/api/rma.html#nvshmem-get) API to get a scalar quantity on the remote PE.\n",
    "\n",
    "```\n",
    "float r = nvshmem_float_g(source, target_pe);\n",
    "```\n",
    "\n",
    "This then looks like the following. Note that with respect to PE 0, location `N / M` corresponds to index `0` of PE 1.\n",
    "\n",
    "```\n",
    "f_left = f[N / M - 2]\n",
    "f_right = nvshmem_float_g(&f[0], 1)\n",
    "f[N / M - 1] = (f_right + f_left) / 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let is to check the code `jacobi_nvshmem.cpp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing jacobi_nvshmem.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile jacobi_nvshmem.cu\n",
    "#include <iostream>\n",
    "#include <limits>\n",
    "#include <cstdio>\n",
    "#include <nvshmem.h>\n",
    "#include <nvshmemx.h>\n",
    "#define NUM_POINTS 4194304\n",
    "#define TOLERANCE  0.0001\n",
    "#define MAX_ITERS  1000\n",
    "\n",
    "__global__ void jacobi (const float* f_old, float* f, float* l2_norm, int N)\n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    int my_pe = nvshmem_my_pe();\n",
    "    int n_pes = nvshmem_n_pes();\n",
    "\n",
    "    // Don't participate if we're the leftmost PE and on\n",
    "    // the leftmost boundary point, or if we're the rightmost\n",
    "    // PE and on the rightmost boundary point (as these are fixed).\n",
    "    bool on_boundary = false;\n",
    "\n",
    "    if (my_pe == 0 && idx == 0) \n",
    "    {\n",
    "        on_boundary = true;\n",
    "    }\n",
    "    else if (my_pe == n_pes - 1 && idx == N - 1) \n",
    "    {\n",
    "        on_boundary = true;\n",
    "    }\n",
    "\n",
    "    if (idx < N && !on_boundary) {\n",
    "        // Retrieve the left and right points in the old data.\n",
    "        // If we're fully in the interior of the local domain,\n",
    "        // this is fully a local access. Otherwise, we need to\n",
    "        // reach out to the remote PE to the left or right.\n",
    "\n",
    "        float f_left;\n",
    "        float f_right;\n",
    "\n",
    "        if (idx == 0) \n",
    "        {\n",
    "            // Note we don't get here if my_pe == 0.\n",
    "            f_left = nvshmem_float_g(&f_old[N - 1], my_pe - 1);\n",
    "        }\n",
    "        else \n",
    "        {\n",
    "            f_left = f_old[idx - 1];\n",
    "        }\n",
    "\n",
    "        if (idx == N - 1) \n",
    "        {\n",
    "            // Note we don't get here if my_pe == n_pes - 1.\n",
    "            f_right = nvshmem_float_g(&f_old[0], my_pe + 1);\n",
    "        }\n",
    "        else \n",
    "        {\n",
    "            f_right = f_old[idx + 1];\n",
    "        }\n",
    "\n",
    "        f[idx] = 0.5f * (f_right + f_left);\n",
    "\n",
    "        float l2 = (f[idx] - f_old[idx]) * (f[idx] - f_old[idx]);\n",
    "\n",
    "        atomicAdd(l2_norm, l2);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void initialize (float* f, float T_left, float T_right, int N)\n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    int my_pe = nvshmem_my_pe();\n",
    "    int n_pes = nvshmem_n_pes();\n",
    "\n",
    "    if (idx == 0 && my_pe == 0) \n",
    "    {\n",
    "        f[idx] = T_left;\n",
    "    }\n",
    "    else if (idx == N - 1 && my_pe == n_pes - 1) \n",
    "    {\n",
    "        f[idx] = T_right;\n",
    "    }\n",
    "    else if (idx < N - 1) \n",
    "    {\n",
    "        f[idx] = 0.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(int argc, char ** argv) \n",
    "{\n",
    "    // Initialize NVSHMEM\n",
    "    nvshmem_init();\n",
    "\n",
    "    // Obtain our NVSHMEM processing element ID and number of PEs\n",
    "    int my_pe = nvshmem_my_pe();\n",
    "    int n_pes = nvshmem_n_pes();\n",
    "\n",
    "    // Each PE (arbitrarily) chooses the GPU corresponding to its ID\n",
    "    int device = my_pe;\n",
    "    cudaSetDevice(device);\n",
    "\n",
    "    // Each device handles a fraction 1 / n_pes of the work.\n",
    "    const int N = NUM_POINTS / n_pes;\n",
    "\n",
    "    // Allocate space for the grid data, and the temporary buffer\n",
    "    // for the \"old\" data.\n",
    "    float* f_old = (float*) nvshmem_malloc(N * sizeof(float));\n",
    "    float* f = (float*) nvshmem_malloc(N * sizeof(float));\n",
    "\n",
    "    // Allocate memory for the L2 norm, on both the host and device.\n",
    "    float* l2_norm = (float*) malloc(sizeof(float));\n",
    "    float* d_l2_norm = (float*) nvshmem_malloc(sizeof(float));\n",
    "\n",
    "    // Initialize the error to a large number.\n",
    "    float error = std::numeric_limits<float>::max();\n",
    "\n",
    "    // Initialize the data.\n",
    "    int threads_per_block = 256;\n",
    "    int blocks = (N + threads_per_block - 1) / threads_per_block;\n",
    "\n",
    "    float T_left = 5.0f;\n",
    "    float T_right = 10.0f;\n",
    "    initialize<<<blocks, threads_per_block>>>(f_old, T_left, T_right, N);\n",
    "    initialize<<<blocks, threads_per_block>>>(f, T_left, T_right, N);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Now iterate until the error is sufficiently small.\n",
    "    // As a safety mechanism, cap the number of iterations.\n",
    "    int num_iters = 0;\n",
    "\n",
    "    while (error > TOLERANCE && num_iters < MAX_ITERS) \n",
    "    {\n",
    "        // Initialize the norm data to zero\n",
    "        cudaMemset(d_l2_norm, 0, sizeof(float));\n",
    "\n",
    "        // Launch kernel to do the calculation\n",
    "        jacobi<<<blocks, threads_per_block>>>(f_old, f, d_l2_norm, N);\n",
    "        cudaDeviceSynchronize();\n",
    "\n",
    "        // Swap f_old and f\n",
    "        std::swap(f_old, f);\n",
    "\n",
    "        // Sum the L2 norm over all PEs\n",
    "        // Note this is a blocking API, so no explicit barrier is needed afterward\n",
    "        nvshmem_float_sum_reduce(NVSHMEM_TEAM_WORLD, d_l2_norm, d_l2_norm, 1);\n",
    "\n",
    "        // Update norm on host; calculate error by normalizing by number of zones and take square root\n",
    "        cudaMemcpy(l2_norm, d_l2_norm, sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "        if (*l2_norm == 0.0f) \n",
    "        {\n",
    "            // Deal with first iteration\n",
    "            error = 1.0f;\n",
    "        }\n",
    "        else \n",
    "        {\n",
    "            error = std::sqrt(*l2_norm / NUM_POINTS);\n",
    "        }\n",
    "\n",
    "        if (num_iters % 10 == 0 && my_pe == 0) \n",
    "        {\n",
    "            std::cout << \"Iteration = \" << num_iters << \" error = \" << error << std::endl;\n",
    "        }\n",
    "\n",
    "        ++num_iters;\n",
    "    }\n",
    "\n",
    "    if (my_pe == 0) \n",
    "    {\n",
    "        if (error <= TOLERANCE && num_iters < MAX_ITERS) \n",
    "        {\n",
    "            std::cout << \"Success!\\n\";\n",
    "        }\n",
    "        else \n",
    "        {\n",
    "            std::cout << \"Failure!\\n\";\n",
    "        }\n",
    "    }\n",
    "\n",
    "    free(l2_norm);\n",
    "    nvshmem_free(d_l2_norm);\n",
    "    nvshmem_free(f_old);\n",
    "    nvshmem_free(f);\n",
    "\n",
    "    // Finalize nvshmem\n",
    "    nvshmem_finalize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run the Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -rdc=true -ccbin g++ -gencode=arch=compute_70,code=sm_70 -I $NVSHMEM_HOME/include jacobi_nvshmem.cu -o jacobi_nvshmem -L $NVSHMEM_HOME/lib -lnvshmem -lnvidia-ml -lcuda -lcudart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 0 error = 0.00272958\n",
      "Iteration = 10 error = 0.00034546\n",
      "Iteration = 20 error = 0.000210903\n",
      "Iteration = 30 error = 0.000157015\n",
      "Iteration = 40 error = 0.000127122\n",
      "Iteration = 50 error = 0.00010783\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "!nvshmrun -np 4 ./jacobi_nvshmem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear the Temporary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf nccl* ping-pong-* *.sh monte_carlo_* nvshmem_pi* jacobi*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
