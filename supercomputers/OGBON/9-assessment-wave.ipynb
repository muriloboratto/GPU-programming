{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your NVSHMEM skills! In this notebook We provide a fully-implemented CUDA application that works on a single GPU, and your job is to convert it to work correctly in NVSHMEM for an arbitrary number of PEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demonstrate your ability to write NVSHMEM code for an arbitrary number of PEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Wave Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Jacobi program, you don't need to really understand this algorithm to work with the code, but, we will take time here to introduce it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we'll solve is the 1D wave equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial t^2} u(x,t) = c^2 \\frac{\\partial^2}{\\partial x^2} u(x,t)\n",
    "$$\n",
    "\n",
    "Here $u = u(x,t)$ is a scalar function of space ($x$) and time ($t$), and $c$ is a (constant) characteristic wave speed (for example, the speed of sound if the wave in question is propagating in air).\n",
    "\n",
    "Implementing the centered-difference discretization of the spatial and time derivatives, one way to write this is:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\Delta t^2} \\left(u_{i}^{n+1} - 2 u_{i}^{n} + u_{i}^{n-1}\\right) = \\frac{c^2}{\\Delta x^2} \\left(u_{i+1}^{n} - 2 u_{i}^{n} + u_{i-1}^{n}\\right)\n",
    "$$\n",
    "\n",
    "Where subscripts denote spatial indices and superscripts denote timesteps. Rearranging this for the unknown $u_{i}^{n+1}$ in terms of known quantities at timesteps $n$ and $n-1$, we have:\n",
    "\n",
    "$$\n",
    "u_{i}^{n+1} = 2 u_{i}^{n} - u_{i}^{n-1} + \\left(\\frac{c\\, \\Delta t}{\\Delta x}\\right)^2 \\left(u_{i+1}^{n} - 2 u_{i}^{n} + u_{i-1}^{n}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve using this method, we simply need to retain the value of the solution at two previous timesteps, and then replace the old data after each update.\n",
    "\n",
    "We're going to specify a 1D domain from $x = 0.0$ to $x = 1.0$, and discretize into $N$ points with a grid spacing of $\\Delta x = 1.0\\, /\\, (N - 1)$. $\\Delta t$ [must be chosen](https://en.wikipedia.org/wiki/Courant%E2%80%93Friedrichs%E2%80%93Lewy_condition) so that it is less than or equal to $c \\Delta x$. To simplify, we'll choose $c = 1.0$ so that we don't have to worry about that term floating around.\n",
    "\n",
    "We're going to specify that $u(0, t) = u(1, t) = 0$. We can think of this like we're solving a wave propagating in a string, where the two ends of the string are held taut. What we specify is the initial condition $u(x, 0)$, a simple sine wave, as well as an initial condition (the velocity at $t = 0$ is zero, which is effectively implemented by starting with $u^{n} == u^{n-1}$).\n",
    "\n",
    "The period of this wave is 1.0, so after simulating up to $t = 1$, the wave should return exactly to where it started. We will use that to verify our solution -- the check at the end of the code will print an \"error\" which is the $L^2$ norm of the current solution with respect to the initial solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Single GPU Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is implemented in standard CUDA for a single GPU. Take some time to review the algorithm and its parallel implementation, and examine the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wave-1GPU.cu\n",
    "#include <iostream>\n",
    "#include <limits>\n",
    "#include <cstdio>\n",
    "#include <cmath>\n",
    "\n",
    "// Number of points in the overall spatial domain\n",
    "#define NUM_POINTS 1048576\n",
    "\n",
    "__global__ void wave_update (float* u, const float* u_old, const float* u_older, float dtdxsq, int N)\n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    if (idx > 0 && idx < N - 1) {\n",
    "\n",
    "        float u_old_right = u_old[idx+1];\n",
    "        float u_old_left = u_old[idx-1];\n",
    "\n",
    "        u[idx] = 2.0f * u_old[idx] - u_older[idx] +\n",
    "                 dtdxsq * (u_old_right - 2.0f * u_old[idx] + u_old_left);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void initialize (float* u, int N)\n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    if (idx < N) {\n",
    "        u[idx] = std::sin(2.0f * M_PI * idx / static_cast<float>(NUM_POINTS - 1));\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{\n",
    "    const int N = NUM_POINTS;\n",
    "\n",
    "    // Allocate space for the grid data, and the temporary buffer\n",
    "    // for the \"old\" and \"older\" data.\n",
    "    float* u_older;\n",
    "    float* u_old;\n",
    "    float* u;\n",
    "\n",
    "    cudaMalloc(&u_older, N * sizeof(float));\n",
    "    cudaMalloc(&u_old, N * sizeof(float));\n",
    "    cudaMalloc(&u, N * sizeof(float));\n",
    "\n",
    "    // Initialize the data\n",
    "    int threads_per_block = 256;\n",
    "    int blocks = (N + threads_per_block - 1) / threads_per_block;\n",
    "\n",
    "    initialize<<<blocks, threads_per_block>>>(u_older, N);\n",
    "    initialize<<<blocks, threads_per_block>>>(u_old, N);\n",
    "    initialize<<<blocks, threads_per_block>>>(u, N);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Now iterate until we've completed a full period\n",
    "    const float period = 1.0f;\n",
    "    const float start_time = 0.0f;\n",
    "    const float stop_time = period;\n",
    "\n",
    "    // Maximum stable timestep is <= dx\n",
    "    float stability_factor = 0.5f;\n",
    "    float dx = 1.0f / (NUM_POINTS - 1);\n",
    "    float dt = stability_factor * dx;\n",
    "\n",
    "    float t = start_time;\n",
    "    const float safety_factor = (1.0f - 1.0e-5f);\n",
    "\n",
    "    int num_steps = 0;\n",
    "\n",
    "    while (t < safety_factor * stop_time) {\n",
    "        // Make sure the last step does not go over the target time\n",
    "        if (t + dt >= stop_time) {\n",
    "            dt = stop_time - t;\n",
    "        }\n",
    "\n",
    "        float dtdxsq = (dt / dx) * (dt / dx);\n",
    "\n",
    "        // Launch kernel to do the calculation\n",
    "        wave_update<<<blocks, threads_per_block>>>(u, u_old, u_older, dtdxsq, N);\n",
    "        cudaDeviceSynchronize();\n",
    "\n",
    "        // Swap u_old and u_older\n",
    "        std::swap(u_old, u_older);\n",
    "\n",
    "        // Swap u and u_old\n",
    "        std::swap(u, u_old);\n",
    "\n",
    "        // Print out diagnostics periodically\n",
    "        if (num_steps % 100000 == 0) {\n",
    "            std::cout << \"Current integration time = \" << t << \"\\n\";\n",
    "        }\n",
    "\n",
    "        // Update t\n",
    "        t += dt;\n",
    "        ++num_steps;\n",
    "    }\n",
    "\n",
    "    // Clean up\n",
    "    cudaFree(u_older);\n",
    "    cudaFree(u_old);\n",
    "    cudaFree(u);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Run the Code for 1GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -x cu -arch=sm_70 -o wave-1GPU wave-1GPU.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "!./wave-1GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment: Implement with NVSHMEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement this with NVSHMEM, dividing the domain equally into $M$   subdomains (for $M$   PEs), with PE 0 owning $x = [0, 1 / M]$  , PE 0 owning $x = [1/M, 2/M]$  , etc. Keep the points at $x = 0$   and $x = 1$   completely fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to keep in mind as you're implementing your solution:\n",
    "- Currently the initialization routine assumes the full domain is available. You'll have to modify that so that each PE sets the appropriate initial conditions for its location in the spatial domain.\n",
    "- Similarly, for the solution check, make sure you do this properly across all PEs by summing up the L2 norm locally and then reducing all over PEs.\n",
    "- You'll have to do some point-to-point communication inside the actual `wave_update()` routine to get the \"halo\" data from neighboring PEs (but be careful not to update the boundary points, one of which lives on PE 0, the other on PE $M$-1).\n",
    "- It's OK if your solution is not any faster than the single-GPU case. We are mostly focusing on writing correct NVSHMEM code in this lab. Given the default value of `NUM_POINTS`, the amount of work per kernel is small enough that we're not really efficiently utilizing the GPU. If you want to do a more realistic performance comparison, set `NUM_POINTS` to a much larger number (but then cap the integration to, say, 10000 steps so that it completes in a reasonable amount of time). You can use the Jupyter notebook `%time` magic function to easily compare application runtime (e.g. `%time !nvshmrun -np 4 ...`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix the code wave.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are really struggling, you can execute the following cell to copy a near-solution implementation into `wave.cpp` that will still require you to address some `FIXMEs` to complete the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wave.cpp\n",
    "#include <iostream>\n",
    "#include <limits>\n",
    "#include <cstdio>\n",
    "#include <cmath>\n",
    "\n",
    "#include <nvshmem.h>\n",
    "#include <nvshmemx.h>\n",
    "\n",
    "// Number of points in the overall spatial domain\n",
    "#define NUM_POINTS 1048576\n",
    "\n",
    "__global__ void wave_update (float* u, const float* u_old, const float* u_older, float dtdxsq, int N)\n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    int my_pe = // FIXME\n",
    "    int n_pes = // FIXME\n",
    "\n",
    "    bool on_boundary = false;\n",
    "    if (my_pe == 0 && idx == 0) {\n",
    "        on_boundary = true;\n",
    "    }\n",
    "    else if (my_pe == n_pes - 1 && idx == N-1) {\n",
    "        on_boundary = true;\n",
    "    }\n",
    "\n",
    "    if (idx < N && !on_boundary) {\n",
    "        float u_old_left;\n",
    "\n",
    "        if (idx == 0) {\n",
    "            // Note we do not get here if we're PE == 0\n",
    "            \n",
    "            // FIXME: Get `u_old_left` from the last element of the previous PE.\n",
    "            // u_old_left = nvshmem_float_g(TODO);\n",
    "               u_old_left = // FIXME\n",
    "        }\n",
    "        else {\n",
    "            u_old_left = u_old[idx-1];\n",
    "        }\n",
    "\n",
    "        float u_old_right;\n",
    "        if (idx == N-1) {\n",
    "            // Note we do not get here if we're PE == n_pes - 1\n",
    "            \n",
    "            // FIXME: Get `u_old_right` from the first element of the next PE.\n",
    "            // u_old_right = nvshmem_float_g(TODO);\n",
    "            u_old_right = // FIXME\n",
    "        }\n",
    "        else {\n",
    "            u_old_right = u_old[idx+1];\n",
    "        }\n",
    "\n",
    "        u[idx] = 2.0f * u_old[idx] - u_older[idx] +\n",
    "                 dtdxsq * (u_old_right - 2.0f * u_old[idx] + u_old_left);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void initialize (float* u, int N)\n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    int offset = nvshmem_my_pe() * (NUM_POINTS / nvshmem_n_pes());\n",
    "\n",
    "    if (idx < N) {\n",
    "        u[idx] = std::sin(2.0f * M_PI * (idx + offset) / static_cast<float>(NUM_POINTS - 1));\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "int main() {\n",
    "    // FIXME: Initialize NVSHMEM\n",
    "    \n",
    "    \n",
    "    // FIXME: Obtain our NVSHMEM processing element ID and number of PEs\n",
    "    int my_pe = // FIXME\n",
    "    int n_pes = // FIXME \n",
    "\n",
    "    // Each PE (arbitrarily) chooses the GPU corresponding to its ID\n",
    "    int device = my_pe;  \n",
    "    cudaSetDevice(device);\n",
    "\n",
    "    // Set `N` to total number of points over number of PEs\n",
    "    const int N = NUM_POINTS / n_pes; // FIXME \n",
    "\n",
    "    // Allocate symmetric data for the grid data, and the temporary buffer\n",
    "    // for the \"old\" and \"older\" data.\n",
    "     float* u_older = (float*) nvshmem_malloc( N * sizeof(float)); \n",
    "     float* u_old =   (float*) nvshmem_malloc( N * sizeof(float)); \n",
    "     float* u =       (float*) nvshmem_malloc( N * sizeof(float)); \n",
    "\n",
    "    // Initialize the data\n",
    "    int threads_per_block = 256;\n",
    "    int blocks = (N + threads_per_block - 1) / threads_per_block;\n",
    "\n",
    "    initialize<<<blocks, threads_per_block>>>(u_older, N);\n",
    "    initialize<<<blocks, threads_per_block>>>(u_old, N);\n",
    "    initialize<<<blocks, threads_per_block>>>(u, N);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Now iterate until we've completed a full period\n",
    "    const float period = 1.0f;\n",
    "    const float start_time = 0.0f;\n",
    "    const float stop_time = period;\n",
    "\n",
    "    // Maximum stable timestep is <= dx\n",
    "    float stability_factor = 0.5f;\n",
    "    float dx = 1.0f / (NUM_POINTS - 1);\n",
    "    float dt = stability_factor * dx;\n",
    "\n",
    "    float t = start_time;\n",
    "    const float safety_factor = (1.0f - 1.0e-5f);\n",
    "\n",
    "    int num_steps = 0;\n",
    "\n",
    "    while (t < safety_factor * stop_time) {\n",
    "        // Make sure the last step does not go over the target time\n",
    "        if (t + dt >= stop_time) {\n",
    "            dt = stop_time - t;\n",
    "        }\n",
    "\n",
    "        float dtdxsq = (dt / dx) * (dt / dx);\n",
    "\n",
    "        // Launch kernel to do the calculation\n",
    "        wave_update<<<blocks, threads_per_block>>>(u, u_old, u_older, dtdxsq, N);\n",
    "        cudaDeviceSynchronize();\n",
    "\n",
    "        // FIXME: Synchronize all PEs before peforming the swaps.\n",
    "        nvshmem_barrier_all();\n",
    "        // Swap u_old and u_older\n",
    "        std::swap(u_old, u_older);\n",
    "\n",
    "        // Swap u and u_old\n",
    "        std::swap(u, u_old);\n",
    "\n",
    "        // Print out diagnostics periodically\n",
    "        // FIXME: Only do the periodic print if this is PE 0.\n",
    "        if (num_steps % 100000 == 0 && my_pe == 0) {\n",
    "            std::cout << \"Current integration time = \" << t << \"\\n\";\n",
    "        }\n",
    "\n",
    "        // Update t\n",
    "        t += dt;\n",
    "        ++num_steps;\n",
    "    }\n",
    "\n",
    "    // Clean up\n",
    "    // FIXME: Use NVSHMEM to free `u_older`, `u_old`, and `u`.\n",
    "    \n",
    "    \n",
    "    // FIXME: Finalize NVSHMEM\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Code for NVSHMEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -x cu -arch=sm_70 -rdc=true -I $NVSHMEM_HOME/include -L $NVSHMEM_HOME/lib -lnvshmem -lcuda -o wave wave.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "!nvshmrun -np $NUM_DEVICES ./wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've now mastered the fundamentals of NCCL, CUDA-aware MPI, and NVSHMEM and are ready to begin applying it in your own problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
