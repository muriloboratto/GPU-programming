{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collective Communication Operations with NCCL on Multi-GPU Systems - Multiple GPUs with Peer-to-Peer Comunications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will introduce direct peer-to-peer memory access across GPUs, and refactor the multi-GPU code from the previous notebook to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understand how to check for and enable direct peer-to-peer memory for applications running on multiple GPUs.\n",
    "- Understant the API NCCL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective Communication Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to carry out communication operations involving multiple computational resources in most parallel applications. These communication operations can be implemented through point-to-point operations. However, this approach could be more efficient for the programmer. Parallel and distributed solutions based on collective operations have long been chosen for these applications. The MPI pattern has very efficient routines that perform collective operations, taking better advantage of the computing power of the available computational resources. Likewise, with the advent of new computational resources, similar routines appear for multi-GPU systems, for example,  [NCCL (NVIDIA Collective Communications Library)](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html). This notebook will cover handling these routines for multi-GPU environments, constantly comparing them with the MPI standard and showing the differences and similarities between the two computational execution environments.\n",
    "\n",
    "One of the main characteristics of using these types of operations is that communications can have different symmetries and asynchronous, considering factors such as emission and reception. For collective operations, symmetry is defined as the characteristic that all involved resources have to perform the same functions with similar parameters. Asynchronous is defined as the inherent characteristic that all the detailed resources have of not waiting for the others to finish to continue the execution. Considering that collective operations are communication patterns that affect all computational resources of an execution group, it was possible to compare aspects of collective operations using multiprocessor and multi-GPU systems, described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Broadcast` is a collective operation where a computational resource sends the same information to all other elements of an execution group. In NCCL the responsible function that performs this operation is __ncclBcast__, being practically equivalent to the __MPI_Bcast__ function of the MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    ncclBcast(void* buff,                        \n",
    "              size_t count,                      \n",
    "              ncclDataType_t datatype,                     \n",
    "              int root,                                    \n",
    "              ncclComm_t comm,                             \n",
    "              cudaStream_t stream                          \n",
    "              );\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure above shows the comparative structure scheme of the functions __ncclBcast__. The function is practically identical to the __MPI_Bcast__. All resources must invoke both in the __comm__ communicator group. Parts send the information stored in __buff__ of the __root__ resource to everyone else belonging to the execution group. The parameters __count__ and __datatype__ have the functions of specifying the amount of memory that the resource __root__ should send to others and the space they should reserve to store the received message. The only difference between the two approaches is found in the last parameter called __stream__, which represents the sending format between the GPUs, so the operator structures are maintained through variable-sized memory spaces in buffer shipping.\n",
    "\n",
    "The collective operation of `Broadcast` using NCCL with the __ncclBcast__ function allows disseminating information to all communicators of a CUDA application, starting from the identifier GPU 0. The function has the same parameters (__count__, __datatype__, __root__ and __comm__) as __MPI_Bcast__ add the parameter __stream__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ncclBcast.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <nccl.h>\n",
    " \n",
    "__global__ void kernel(int *a) \n",
    "{\n",
    "  int index = threadIdx.x;\n",
    "  a[index] *= 2;\n",
    "  printf(\"%d\\t\", a[index]);\n",
    "}\n",
    " \n",
    "void printVector(int *in, int n)\n",
    "{\n",
    " printf(\"\\nThis is the host\\n\");\n",
    " for(int i = 0; i < n; i++)\n",
    "  printf(\"%d\\t\", in[i]);\n",
    " printf(\"\\n\");\n",
    "}\n",
    "\n",
    "int main(int argc, char* argv[]) \n",
    "{\n",
    "  /*Variables*/\n",
    "  int dataSize = 8;\n",
    "  int nGPUs = 0;\n",
    "  cudaGetDeviceCount(&nGPUs); \n",
    "  printf(\"nGPUs = %d\\n\",nGPUs);\n",
    "    \n",
    "  int *h_data       = (int*)  malloc (dataSize  * sizeof(int));\n",
    "  int **d_data      = (int**) malloc (nGPUs     * sizeof(int*));\n",
    "  \n",
    "  int *DeviceList   = (int *) malloc (nGPUs     * sizeof(int));\n",
    "  for(int i = 0; i < nGPUs; i++)\n",
    "      DeviceList[i] = i;\n",
    "  \n",
    "  /*Initializing NCCL with Multiples Devices per Thread*/\n",
    "  ncclComm_t* comms      = (ncclComm_t*)  malloc(sizeof(ncclComm_t)  * nGPUs);  \n",
    "  cudaStream_t* stream   = (cudaStream_t*)malloc(sizeof(cudaStream_t)* nGPUs);\n",
    "  ncclCommInitAll(comms, nGPUs, DeviceList);\n",
    "  \n",
    "  /*Population the data vector*/\n",
    "  for(int i = 0; i < dataSize; i++)\n",
    "      h_data[i] = rand()%(10-2)*2;\n",
    " \n",
    "  printVector(h_data, dataSize);\n",
    "      \n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "      cudaSetDevice(g);\n",
    "      cudaStreamCreate(&stream[g]);\n",
    "      cudaMalloc(&d_data[g], dataSize * sizeof(int));\n",
    "     \n",
    "      if(g == 0)  /*Copy from Host to Device*/\n",
    "         cudaMemcpy(d_data[g], h_data, dataSize * sizeof(int), cudaMemcpyHostToDevice);\n",
    "  }\n",
    "        \n",
    "  ncclGroupStart();\n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "    cudaSetDevice(DeviceList[g]);\n",
    "    ncclBcast(d_data[g], dataSize, ncclInt, 0, comms[g], stream[g]); /*Broadcasting it to all*/\n",
    "  }\n",
    "  ncclGroupEnd();       \n",
    "\n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "    cudaSetDevice(DeviceList[g]);\n",
    "    printf(\"\\nThis is the device [%d]\\n\", g);\n",
    "    kernel <<< 1 , dataSize >>> (d_data[g]); /*Call the CUDA Kernel*/\n",
    "    cudaDeviceSynchronize();             \n",
    "  }\n",
    "\n",
    "  printf(\"\\n\");\n",
    "    \n",
    "  for(int g = 0; g < nGPUs; g++)  /*Synchronizing CUDA Streams*/\n",
    "  {                                \n",
    "    cudaSetDevice(DeviceList[g]);\n",
    "    cudaStreamSynchronize(stream[g]);\n",
    "  }\n",
    " \n",
    "  for(int g = 0; g < nGPUs; g++)   /*Destroy CUDA Streams*/\n",
    "  {                                \n",
    "    cudaSetDevice(DeviceList[g]);\n",
    "    cudaStreamDestroy(stream[g]);\n",
    "  }\n",
    "\n",
    "  for(int g = 0; g < nGPUs; g++)   /*Finalizing NCCL*/\n",
    "     ncclCommDestroy(comms[g]);\n",
    "  \n",
    "  /*Freeing memory*/\n",
    "  free(h_data); \n",
    "  free(DeviceList);\n",
    "  cudaFree(stream);\n",
    "  cudaFree(d_data);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows you to `Broadcast` information to multiple GPUs that are on the same execution group, which follows the scheme in the follow figure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bcast.png\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " module load nccl/2.13_cuda-11.2\n",
    " nvcc ncclBcast.cu -o ncclBcast -lnccl $CPPFLAGS $LDFLAGS\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile v100-ncclBcast.sh\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=ncclBcast                   # Job name\n",
    "#SBATCH --nodes=1                              # Run on 1 node  \n",
    "#SBATCH --partition=sequana_gpu_dev            # Partition SDUMONT\n",
    "#SBATCH --output=out_v100_%j-ncclBcast.log     # Standard output and error log\n",
    "#SBATCH --ntasks-per-node=1                    # 1 job per node\n",
    "\n",
    "module load nccl/2.13_cuda-11.2\n",
    "./ncclBcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " sbatch v100-ncclBcast.sh\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtoexecute.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print output in log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat *-ncclBcast.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Reduce` is a collective operation where each computational resource involved contributes an operand to perform the global calculation of an associative or commutative operation (i.e., maximum, minimum, sum, product, etc). In NCCL, the responsible function that performs this operation is __ncclReduce__, being equivalent to the __MPI\\_Reduce__ function of the MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "     ncclReduce(const void* sendbuff,           \n",
    "                void* recvbuff,                            \n",
    "                size_t count,                              \n",
    "                ncclDataType_t datatype,                   \n",
    "                ncclRedOp_t op,                            \n",
    "                int root,                                  \n",
    "                ncclComm_t comm,                           \n",
    "                cudaStream_t stream                      \n",
    "                );\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure above shows the comparative scheme of the functions __ncclReduce__ and your similitude with the command __MPI_Reduce__. As before, the two functions are identical. In the *Reduce* operation, the operator is applied to the data located in each computational resource's send buffer (__sendbuff__). The result of this function is passed back to all resources in the receive buffer (__recbuff__). The __count__ and __datatype__ parameters again specify how much memory the __root__ resource should send to others and how much space they should reserve to store the incoming message. And again, the difference is in the last parameter called __stream__, which represents the format of the sending buffer between the GPUs.\n",
    "\n",
    "To show an example of the `Reduce` function, we will use the example of the dot product [dot product](https://en.wikipedia.org/wiki/Dot_product) of vectors. The code will distribute the scalar product of two vectors *x* and *y*, as shown below. First, the partial result $(x * y)$ is calculated, and then the `Reduce` operation is performed. After the operation, the final result will be stored in the source resource, called resource 0. This reduce operation is performed in NCCL by the __ncclReduce__ function on multi-GPU systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ncclReduce.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <nccl.h>\n",
    "\n",
    "__global__ void Dev_dot(double *x, double *y, int n) \n",
    "{   \n",
    "   __shared__ double tmp[512];\n",
    "\n",
    "   int i = threadIdx.x;\n",
    "   int t = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "   \n",
    "   if (t < n) \n",
    "    tmp[i] = x[t];\n",
    "   \n",
    "   __syncthreads();\n",
    "\n",
    "   for (int stride = blockDim.x / 2; stride >  0; stride /= 2) \n",
    "   {\n",
    "\n",
    "      if (i < stride)\n",
    "         tmp[i] += tmp[i + stride];\n",
    "\n",
    "      __syncthreads();\n",
    "\n",
    "   }\n",
    "\n",
    "   if (threadIdx.x == 0) \n",
    "   {\n",
    "      y[blockIdx.x] = tmp[0];\n",
    "      printf(\"dot(x,y) = %1.2f\\n\", y[blockIdx.x]); \n",
    "   }\n",
    "}    \n",
    "\n",
    "__global__ void Dev_print(double *x) \n",
    "{   \n",
    "  int i = threadIdx.x;\n",
    "  printf(\"%1.2f\\t\", x[i]);\n",
    "}\n",
    "\n",
    "void printVector(double *in, int n)\n",
    "{\n",
    "  for(int i=0; i < n; i++)\n",
    "    printf(\"%1.2f\\t\", in[i]);\n",
    "  printf(\"\\n\");\n",
    "}\n",
    "\n",
    "\n",
    "int main(int argc, char* argv[]) \n",
    "{\n",
    "  /*Variables*/\n",
    "  int nGPUs = 0;\n",
    "  cudaGetDeviceCount(&nGPUs);\n",
    "  printf(\"nGPUs = %d\\n\",nGPUs);\n",
    "    \n",
    "  int dataSize = 8;\n",
    "  double *x          = (double*)    malloc(dataSize  * sizeof(double));\n",
    "  double *y          = (double*)    malloc(dataSize  * sizeof(double)); \n",
    "  double **x_d_data  = (double**)   malloc(nGPUs     * sizeof(double*));\n",
    "  double **y_d_data  = (double**)   malloc(nGPUs     * sizeof(double*));\n",
    "  double **Sx_d_data = (double**)   malloc(nGPUs     * sizeof(double*));\n",
    "  double **Sy_d_data = (double**)   malloc(nGPUs     * sizeof(double*));\n",
    " \n",
    "  int *DeviceList = (int *) malloc (nGPUs * sizeof(int));  \n",
    "  for(int i = 0; i < nGPUs; ++i)\n",
    "      DeviceList[i] = i;\n",
    "  \n",
    "  /*Initializing NCCL with Multiples Devices per Thread*/\n",
    "  ncclComm_t* comms      = (ncclComm_t*)  malloc(sizeof(ncclComm_t)  * nGPUs);  \n",
    "  cudaStream_t* stream   = (cudaStream_t*)malloc(sizeof(cudaStream_t)* nGPUs);\n",
    "  ncclCommInitAll(comms, nGPUs, DeviceList);\n",
    "      \n",
    "  /*Population vectors*/\n",
    "  for(int i = 0; i < dataSize; i++)\n",
    "  { \n",
    "    x[i] = 1;                \n",
    "    y[i] = 2;\n",
    "  }                \n",
    "  \n",
    "  printf(\"\\nThis is the host\\n\");      \n",
    "  printVector(x, dataSize); \n",
    "  printVector(y, dataSize);\n",
    "    \n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "    cudaSetDevice(DeviceList[g]);\n",
    "    cudaStreamCreate(&stream[g]);\n",
    "\n",
    "    cudaMalloc(&x_d_data[g],    dataSize * sizeof(double));\n",
    "    cudaMalloc(&y_d_data[g],    dataSize * sizeof(double));  \n",
    "    cudaMalloc(&Sx_d_data[g],   dataSize * sizeof(double));\n",
    "    cudaMalloc(&Sy_d_data[g],   dataSize * sizeof(double));\n",
    "     \n",
    "    cudaMemcpy(x_d_data[g],  x, dataSize * sizeof(double), cudaMemcpyHostToDevice); /*Copy x from Host to Devices*/\n",
    "    cudaMemcpy(y_d_data[g],  y, dataSize * sizeof(double), cudaMemcpyHostToDevice); /*Copy y from Host to Devices*/      \n",
    "  }\n",
    "      \n",
    "  ncclGroupStart(); \n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "    cudaSetDevice(DeviceList[g]);\n",
    "    ncclReduce(x_d_data[g], Sx_d_data[g], dataSize, ncclDouble, ncclSum, 0, comms[g], stream[g]); /*Reducing x vector*/\n",
    "    ncclReduce(y_d_data[g], Sy_d_data[g], dataSize, ncclDouble, ncclSum, 0, comms[g], stream[g]); /*Reducing y vector*/\n",
    "  }\n",
    "  ncclGroupEnd(); \n",
    "\n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "    cudaSetDevice(DeviceList[g]);            \n",
    "    printf(\"\\nThis is the device [%d]\\n\", g);\n",
    "    Dev_dot <<< 1, dataSize >>> (Sy_d_data[g], Sx_d_data[g], dataSize); /*Call the CUDA Kernel*/\n",
    "    cudaDeviceSynchronize();  \n",
    "  }\n",
    "  \n",
    "  for(int g = 0; g < nGPUs; g++)  /*Synchronizing CUDA Streams*/\n",
    "  { \n",
    "    cudaSetDevice(DeviceList[g]);\n",
    "    cudaStreamSynchronize(stream[g]);\n",
    "  }\n",
    "  \n",
    "  for(int g = 0; g < nGPUs; g++)  /*Destroy CUDA Streams*/\n",
    "  {  \n",
    "    cudaSetDevice(DeviceList[g]);\n",
    "    cudaStreamDestroy(stream[g]);\n",
    "  }\n",
    "\n",
    "  for(int g = 0; g < nGPUs; g++)  /*Finalizing NCCL*/\n",
    "     ncclCommDestroy(comms[g]);\n",
    "  \n",
    "  /*Freeing memory*/\n",
    "  free(x);\n",
    "  free(y);\n",
    "  free(DeviceList);\n",
    "    \n",
    "  cudaFree(stream);  \n",
    "  cudaFree(x_d_data);\n",
    "  cudaFree(y_d_data);\n",
    "  cudaFree(Sx_d_data);\n",
    "  cudaFree(Sy_d_data);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows you to `Reduce` information to multiple GPUs that are on the same execution group, which follows the scheme in the follow figure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/reduce.png\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " module load nccl/2.13_cuda-11.2\n",
    " nvcc ncclReduce.cu -o ncclReduce -lnccl $CPPFLAGS $LDFLAGS\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile v100-ncclReduce.sh\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=ncclReduce                  # Job name\n",
    "#SBATCH --nodes=1                              # Run on 1 node  \n",
    "#SBATCH --partition=sequana_gpu_dev            # Partition SDUMONT\n",
    "#SBATCH --output=out_v100_%j-ncclReduce.log    # Standard output and error log\n",
    "#SBATCH --ntasks-per-node=1                    # 1 job per node\n",
    "\n",
    "module load nccl/2.13_cuda-11.2\n",
    "./ncclReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " sbatch v100-ncclReduce.sh\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtoexecute.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print output in log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat *-ncclReduce.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Gather` operation is a collective operation where a computational resource scans information from a set of resources. From one point of view, the `Gather` operation is the inverse of the `Scatter` operation. The difference for this last one resides in a combination of data from a receiving resource, which is solely stored. The syntax of the `Gather` function for GPUs corresponds to the __ncclAllGather__ command, which is related to the concept of `AllGather`, which is a routine invocation equivalent to performing *n* calls to the operation, which each time acts as __root__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "      ncclAllGather(const void* sendbuff,           \n",
    "                    void* recvbuff,                               \n",
    "                    size_t sendcount,                             \n",
    "                    ncclDataType_t datatype,                      \n",
    "                    ncclComm_t comm,                              \n",
    "                    cudaStream_t stream                           \n",
    "                    );\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All communicator __comm__ processes send the content __sendbuff__ to the process with identifier __root__. It concatenates all received data ordered by the sender identifier, starting from the position pointed by __recvbuff__. Process data with identifier 0 are stored before resources 1, and so on. The receive arguments __recvdbuff__, __recvcount__, __recvtype__, only have meaning for the __root__ resource. In NCCL, this information is implicit in the function arguments. Only add __stream__ arguments as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ncclAllGather.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <nccl.h>\n",
    "\n",
    "__global__ void Dev_print(float *x) \n",
    "{   \n",
    "   int i = threadIdx.x; \n",
    "   printf(\"%1.2f\\t\", x[i]); \n",
    "}\n",
    "\n",
    "void printVector(float *in, int n)\n",
    "{\n",
    "  for(int i=0; i < n; i++)\n",
    "    if(in[i])\n",
    "     printf(\"%1.2f\\t\", in[i]);\n",
    "}\n",
    "\n",
    "int main(int argc, char* argv[])\n",
    "{\n",
    "  /*Variables*/\n",
    "  int nGPUs = 0;\n",
    "  cudaGetDeviceCount(&nGPUs);\n",
    "  printf(\"nGPUs = %d\\n\",nGPUs);  \n",
    "  int sendcount = 1;\n",
    "  int size      = nGPUs;\n",
    "      \n",
    "  int *DeviceList = (int *) malloc (nGPUs * sizeof(int));\n",
    "  for(int i = 0; i < nGPUs; ++i)\n",
    "      DeviceList[i] = i;\n",
    "    \n",
    " /*Initializing NCCL with Multiples Devices per Thread*/\n",
    "  ncclComm_t* comms      = (ncclComm_t*)  malloc(sizeof(ncclComm_t)  * nGPUs);  \n",
    "  cudaStream_t* stream   = (cudaStream_t*)malloc(sizeof(cudaStream_t)* nGPUs);\n",
    "  ncclCommInitAll(comms, nGPUs, DeviceList);\n",
    "\n",
    "  /*Allocating and initializing device buffers*/\n",
    "  float** sendbuff = (float**) malloc(nGPUs * sizeof(float*));\n",
    "  float** recvbuff = (float**) malloc(nGPUs * sizeof(float*));\n",
    "\n",
    "  /*Host vectors*/ \n",
    "  float host_x0[4] = { 10,   0,  0,  0};\n",
    "  float host_x1[4] = {  0,  20,  0,  0};\n",
    "  float host_x2[4] = {  0,   0, 30,  0};\n",
    "  float host_x3[4] = {  0,   0,  0,  40};\n",
    "    \n",
    "  printf(\"\\nThis is the host\\n\");      \n",
    "  printVector(host_x0, size); \n",
    "  printVector(host_x1, size);\n",
    "  printVector(host_x2, size);\n",
    "  printVector(host_x3, size);  \n",
    "  printf(\"\\n\");  \n",
    "\n",
    "  for(int i = 0; i < nGPUs; ++i) \n",
    "  {\n",
    "    cudaSetDevice(i);\n",
    "    cudaMalloc(&sendbuff[i],  size * sizeof(float));\n",
    "    cudaMalloc(&recvbuff[i],  size * sizeof(float));\n",
    "\n",
    "    switch(i) /*Copy from host to devices*/\n",
    "    {\n",
    "      case 0 : cudaMemcpy(sendbuff[i], host_x0,   size * sizeof(float), cudaMemcpyHostToDevice); break; \n",
    "      case 1 : cudaMemcpy(sendbuff[i], host_x1,   size * sizeof(float), cudaMemcpyHostToDevice); break; \n",
    "      case 2 : cudaMemcpy(sendbuff[i], host_x2,   size * sizeof(float), cudaMemcpyHostToDevice); break; \n",
    "      case 3 : cudaMemcpy(sendbuff[i], host_x3,   size * sizeof(float), cudaMemcpyHostToDevice); break; \n",
    "    }\n",
    "\n",
    "    cudaStreamCreate(stream+i);\n",
    "  } \n",
    "\n",
    "  ncclGroupStart();        \n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "   \tcudaSetDevice(g);\n",
    "    ncclAllGather(sendbuff[g] + g, recvbuff[g], sendcount, ncclFloat, comms[g], stream[g]); /*All Gathering the data on GPUs*/\n",
    "  }\n",
    "  ncclGroupEnd();\n",
    "\n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "    cudaSetDevice(g); \n",
    "    printf(\"\\nThis is the device [%d]\\n\", g);\n",
    "    Dev_print <<< 1, size >>> (recvbuff[g]); /*Call the CUDA Kernel*/\n",
    "    cudaDeviceSynchronize();    \n",
    "  }\n",
    "  printf(\"\\n\");\n",
    "\n",
    "  for(int i = 0; i < nGPUs; ++i) /*Synchronizing CUDA Streams*/\n",
    "  {                                  \n",
    "   cudaSetDevice(i);\n",
    "   cudaStreamSynchronize(stream[i]);\n",
    "  }\n",
    "\n",
    "  for(int i = 0; i < nGPUs; ++i) /*Destroy CUDA Streams*/ \n",
    "  { \n",
    "   cudaSetDevice(i);\n",
    "   cudaFree(sendbuff[i]);\n",
    "   cudaFree(recvbuff[i]);\n",
    "  }\n",
    "\n",
    "  for(int i = 0; i < nGPUs; ++i)   /*Finalizing NCCL*/\n",
    "    ncclCommDestroy(comms[i]);\n",
    "\n",
    " /*Freeing memory*/\n",
    "  free(DeviceList);  \n",
    "  cudaFree(stream);  \n",
    "  cudaFree(sendbuff);\n",
    "  cudaFree(recvbuff);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows you to `Gather` information to multiple GPUs that are on the same execution group, which follows the scheme in the follow figure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/allgather.png\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " module load nccl/2.13_cuda-11.2\n",
    " nvcc ncclAllGather.cu -o ncclAllGather -lnccl $CPPFLAGS $LDFLAGS\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile v100-ncclAllGather.sh\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=ncclAllGather               # Job name\n",
    "#SBATCH --nodes=1                              # Run on 1 node  \n",
    "#SBATCH --partition=sequana_gpu_dev            # Partition SDUMONT\n",
    "#SBATCH --output=out_v100_%j-ncclAllGather.log # Standard output and error log\n",
    "#SBATCH --ntasks-per-node=1                    # 1 job per node\n",
    "\n",
    "module load nccl/2.13_cuda-11.2\n",
    "./ncclAllGather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " sbatch v100-ncclAllGather.sh\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtoexecute.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print output in log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat *-ncclAllGather.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReduceScatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ReduceScatter` operation is a collective operation present in NCCL that merges two operations into one. The `Reduce` operation applies to the `Scatter` operation, which involves a reduction operation by distributing operated blocks among computational resources based on their identifying index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "      ncclReduceScatter(const void* sendbuff,\n",
    "                       void* recvbuff, \n",
    "                       size_t recvcount, \n",
    "                       ncclDataType_t datatype, \n",
    "                       ncclRedOp_t op, \n",
    "                       ncclComm_t comm, \n",
    "                       cudaStream_t stream\n",
    "                       );\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __root__ process concatenates all received data sorted by the sender's range, starting from the position pointed to by __recbuff__. From the position pointed to by __recbbuff__, the __root__ process concatenates all received data ordered by the receiver's interval. That is, the partial data of the lines of all computational resources are stored in a reduced way in the destination resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ncclReduceScatter.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <nccl.h>\n",
    "\n",
    "__global__ void Dev_print(float *x) \n",
    "{   \n",
    "   int i = threadIdx.x; \n",
    "   printf(\"%1.2f\\t\", x[i]); \n",
    "}\n",
    "\n",
    "void printVector(float *in, int n)\n",
    "{\n",
    "  for(int i=0; i < n; i++)\n",
    "   printf(\"%1.2f\\t\", in[i]);\n",
    "  printf(\"\\n\");\n",
    "}\n",
    "\n",
    "int main(int argc, char* argv[])\n",
    "{\n",
    " /*Variables*/\n",
    "  int nGPUs = 0;\n",
    "  cudaGetDeviceCount(&nGPUs);\n",
    "  printf(\"nGPUs = %d\\n\",nGPUs);  \n",
    "  int recvcount = 1;\n",
    "  int size      = nGPUs;   \n",
    "    \n",
    "  int *DeviceList = (int *) malloc (nGPUs * sizeof(int));\n",
    "  for(int i = 0; i < nGPUs; ++i)\n",
    "      DeviceList[i] = i;\n",
    "    \n",
    " /*Initializing NCCL with Multiples Devices per Thread*/\n",
    "  ncclComm_t* comms      = (ncclComm_t*)  malloc(sizeof(ncclComm_t)  * nGPUs);  \n",
    "  cudaStream_t* stream   = (cudaStream_t*)malloc(sizeof(cudaStream_t)* nGPUs);\n",
    "  ncclCommInitAll(comms, nGPUs, DeviceList);\n",
    "\n",
    "  /*Allocating and initializing device buffers*/\n",
    "  float** sendbuff = (float**) malloc(nGPUs * sizeof(float*));\n",
    "  float** recvbuff = (float**) malloc(nGPUs * sizeof(float*));\n",
    "\n",
    "  /*Host vectors*/ \n",
    "  float host_x0[4] = { 10,  50,  90,   130};\n",
    "  float host_x1[4] = { 20,  60,  100,  140};\n",
    "  float host_x2[4] = { 30,  70,  110,  150};\n",
    "  float host_x3[4] = { 40,  80,  120,  160};\n",
    "    \n",
    "  printf(\"\\nThis is the host\\n\");    \n",
    " \n",
    "  if(nGPUs == 4)  \n",
    "  {\n",
    "    printVector(host_x0, size); \n",
    "    printVector(host_x1, size);\n",
    "    printVector(host_x2, size);\n",
    "    printVector(host_x3, size); \n",
    "  }else //nGPUs == 3\n",
    "  {\n",
    "    printVector(host_x0, size); \n",
    "    printVector(host_x1, size);\n",
    "    printVector(host_x2, size);\n",
    "  }  \n",
    "    \n",
    "  for(int i = 0; i < nGPUs; ++i) \n",
    "  {\n",
    "    cudaSetDevice(i);\n",
    "    cudaMalloc(&sendbuff[i],  size * sizeof(float));\n",
    "    cudaMalloc(&recvbuff[i],  size * sizeof(float));\n",
    "\n",
    "    switch(i)  /*Copy from host to devices*/\n",
    "    {\n",
    "      case 0 : cudaMemcpy(sendbuff[i], host_x0,   size * sizeof(float), cudaMemcpyHostToDevice); break; \n",
    "      case 1 : cudaMemcpy(sendbuff[i], host_x1,   size * sizeof(float), cudaMemcpyHostToDevice); break; \n",
    "      case 2 : cudaMemcpy(sendbuff[i], host_x2,   size * sizeof(float), cudaMemcpyHostToDevice); break; \n",
    "      case 3 : cudaMemcpy(sendbuff[i], host_x3,   size * sizeof(float), cudaMemcpyHostToDevice); break; \n",
    "    }\n",
    "      \n",
    "    cudaStreamCreate(stream+i);\n",
    "  } \n",
    "\n",
    "  ncclGroupStart();      \n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "   cudaSetDevice(g);\n",
    "   ncclReduceScatter(sendbuff[g], recvbuff[g], recvcount, ncclFloat, ncclSum, comms[g], stream[g]); /*All Reducing and Scattering the data on GPUs*/   \n",
    "  }\n",
    "  ncclGroupEnd();\n",
    "\n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "    cudaSetDevice(g); \n",
    "    printf(\"\\nThis is the device [%d]\\n\", g);\n",
    "    Dev_print <<< 1, size >>> (recvbuff[g]); /*Call the CUDA Kernel*/\n",
    "    cudaDeviceSynchronize();    \n",
    "  }\n",
    "  printf(\"\\n\");\n",
    "\n",
    "  for (int i = 0; i < nGPUs; ++i)  /*Synchronizing CUDA Streams*/\n",
    "  {                                 \n",
    "   cudaSetDevice(i);\n",
    "   cudaStreamSynchronize(stream[i]);\n",
    "  }\n",
    "\n",
    "  for (int i = 0; i < nGPUs; ++i)  /*Destroy CUDA Streams*/\n",
    "  { \n",
    "   cudaSetDevice(i);\n",
    "   cudaFree(sendbuff[i]);\n",
    "   cudaFree(recvbuff[i]);\n",
    "  }\n",
    "\n",
    "  for(int i = 0; i < nGPUs; ++i)   /*Finalizing NCCL*/\n",
    "    ncclCommDestroy(comms[i]);\n",
    "\n",
    " /*Freeing memory*/\n",
    "  free(DeviceList);\n",
    "    \n",
    "  cudaFree(stream);  \n",
    "  cudaFree(sendbuff);\n",
    "  cudaFree(recvbuff);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows you to `ReduceScatter` information to multiple GPUs that are on the same execution group, which follows the scheme in the follow figure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/reducescatter.png\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " module load nccl/2.13_cuda-11.2\n",
    " nvcc ncclReduceScatter.cu -o ncclReduceScatter -lnccl $CPPFLAGS $LDFLAGS\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile v100-ncclReduceScatter.sh\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=ncclReduceScatter               # Job name\n",
    "#SBATCH --nodes=1                                  # Run on 1 node  \n",
    "#SBATCH --partition=sequana_gpu_dev                # Partition SDUMONT\n",
    "#SBATCH --output=out_v100_%j-ncclReduceScatter.log # Standard output and error log\n",
    "#SBATCH --ntasks-per-node=1                        # 1 job per node\n",
    "\n",
    "module load nccl/2.13_cuda-11.2\n",
    "./ncclReduceScatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " sbatch v100-ncclReduceScatter.sh\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtoexecute.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print output in log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat *-ncclReduceScatter.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peer-to-Peer Communications (P2P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peer-to-peer communication can be used to express any communication pattern between multiple GPUs. Any peer-to-peer communication needs two NCCL calls: one to send the message (__ncclSend__) and the other to receive it (__ncclRecv__), and every message must have the exact count and data typing. Multiple calls to __ncclSend__ and __ncclRecv__ can be combined with __ncclGroupStart__ and __ncclGroupEnd__ to form more complex communication patterns, i.e., the NCCL semantics allow all variants with different sizes, data types, and buffers, by classification, for example: scattering communications, meetings or communication between neighbors in N-dimensional spaces. The syntax of the __ncclSend__ and __ncclRecv__ routines is shown below, as their respective anacronyms in MPI.\n",
    "\n",
    "Peer-to-peer communications within a split will be asymmetric and blocked until the group call is completed. Still, calls within a division can be seen as progressing independently, so they should always be open to each other. Analogous to MPI, a point-to-point operation can be expressed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "   ncclSend(const void* sendbuff,                               ncclRecv(const void* recvbuff,                 \n",
    "            size_t sendcount,                                            size_t recvcount,                            \n",
    "            ncclDataType_t datatype,                                     ncclDataType_t datatype, \n",
    "            int peer,                                                    int peer,             \n",
    "            ncclComm_t comm,                                             ncclComm_t comm,                           \n",
    "            cudaStream_t stream                                          cudaStream_t stream \n",
    "            );                                                           );                                        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ncclSendRecv.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <nccl.h>\n",
    "\n",
    "__global__ void kernel(int *a, int rank) \n",
    "{ \n",
    "  if(rank == 0)\n",
    "    printf(\"%d\\t\", a[threadIdx.x]); \n",
    "      else\n",
    "        printf(\"%d\\t\", a[threadIdx.x] * 10); \n",
    "}\n",
    " \n",
    "void showAll(int *in, int n)\n",
    "{\n",
    "  printf(\"\\nThis is the host\\n\");\n",
    "  for(int i = 0; i < n; i++)\n",
    "    printf(\"%d\\t\", in[i]);     \n",
    "  printf(\"\\n\");\n",
    "}\n",
    "\n",
    "int main(int argc, char* argv[]) \n",
    "{\n",
    "  /*Variables*/  \n",
    "  int size = 8;\n",
    "  int nGPUs = 0;\n",
    "  cudaGetDeviceCount(&nGPUs);\n",
    "  printf(\"nGPUs = %d\\n\",nGPUs);\n",
    "  \n",
    "  int *host       = (int*) malloc(size      * sizeof(int));\n",
    "  int **sendbuff  = (int**)malloc(nGPUs     * sizeof(int*));\n",
    "  int **recvbuff  = (int**)malloc(nGPUs     * sizeof(int*));\n",
    "    \n",
    "  int *DeviceList = (int *) malloc ( nGPUs * sizeof(int));\n",
    "  for(int i = 0; i < nGPUs; ++i)\n",
    "      DeviceList[i] = i;\n",
    "  \n",
    "  /*Initializing NCCL with Multiples Devices per Thread*/\n",
    "  ncclComm_t* comms         = (ncclComm_t*)  malloc(sizeof(ncclComm_t)  * nGPUs);  \n",
    "  cudaStream_t* stream      = (cudaStream_t*)malloc(sizeof(cudaStream_t)* nGPUs);\n",
    "  ncclCommInitAll(comms, nGPUs, DeviceList); \n",
    "\n",
    "  /*Population of vector*/\n",
    "  for(int i = 0; i < size; i++)\n",
    "      host[i] = i + 1;\n",
    "\n",
    "  showAll(host, size);\n",
    "\n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "      cudaSetDevice(DeviceList[g]);\n",
    "      cudaStreamCreate(&stream[g]);\n",
    "      cudaMalloc(&sendbuff[g], size * sizeof(int));\n",
    "      cudaMalloc(&recvbuff[g], size * sizeof(int));\n",
    "     \n",
    "      if(g == 0) /*Copy from host to devices*/\n",
    "        cudaMemcpy(sendbuff[g], host, size * sizeof(int),cudaMemcpyHostToDevice);    \n",
    "  }\n",
    "  \n",
    "   \n",
    "  ncclGroupStart();        \n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "      ncclSend(sendbuff[0], size, ncclInt, g, comms[g], stream[g]);\n",
    "      ncclRecv(recvbuff[g], size, ncclInt, g, comms[g], stream[g]);\n",
    "  }\n",
    "  ncclGroupEnd();          \n",
    "    \n",
    "  for(int g = 0; g < nGPUs; g++) \n",
    "  {\n",
    "      cudaSetDevice(DeviceList[g]);\n",
    "      printf(\"\\nThis is the device [%d]\\n\", g);\n",
    "      \n",
    "      if(g == 0)\n",
    "        kernel <<< 1 , size >>> (sendbuff[g], 0); \n",
    "          else\n",
    "             kernel <<< 1 , size >>> (recvbuff[g], g); \n",
    " \n",
    "      cudaDeviceSynchronize();\n",
    "  }\n",
    "  printf(\"\\n\");\n",
    "\n",
    "  for(int g = 0; g < nGPUs; g++) /*Synchronizing CUDA Streams*/\n",
    "  {\n",
    "    cudaSetDevice(DeviceList[g]);\n",
    "    cudaStreamSynchronize(stream[g]);\n",
    "  }\n",
    " \n",
    "  for(int g = 0; g < nGPUs; g++) /*Destroy CUDA Streams*/\n",
    "  {\n",
    "    cudaSetDevice(DeviceList[g]);\n",
    "    cudaStreamDestroy(stream[g]);\n",
    "  }\n",
    "\n",
    "  for(int g = 0; g < nGPUs; g++) /*Finalizing NCCL*/\n",
    "     ncclCommDestroy(comms[g]);\n",
    "  \n",
    "  /*Freeing memory*/\n",
    "  free(host);\n",
    "  free(DeviceList); \n",
    "    \n",
    "  cudaFree(stream);\n",
    "  cudaFree(sendbuff);\n",
    "  cudaFree(recvbuff);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " module load nccl/2.13_cuda-11.2\n",
    " nvcc ncclSendRecv.cu -o ncclSendRecv -lnccl $CPPFLAGS $LDFLAGS\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile v100-ncclSendRecv.sh\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=ncclSendRecv                # Job name\n",
    "#SBATCH --nodes=1                              # Run on 1 node  \n",
    "#SBATCH --partition=sequana_gpu_dev            # Partition SDUMONT\n",
    "#SBATCH --output=out_v100_%j-ncclSendRecv.log  # Standard output and error log\n",
    "#SBATCH --ntasks-per-node=1                    # 1 job per node\n",
    "\n",
    "module load nccl/2.13_cuda-11.2\n",
    "./ncclSendRecv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " sbatch v100-ncclSendRecv.sh\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtoexecute.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print output in log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat *-ncclSendRecv.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Calculation of PI Number using the Riemann Integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following MPI code, write a parallel program using NCCL that makes use of the collective communication functions`Broadcast` and `Reduce` to calculate the PI number through the Integration of the $\\frac{1}{1+x^2}$, where the Riemann sum approximates the integral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c++\n",
    "#include <stdio.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char **argv) \n",
    "{ \n",
    "  int master = 0, size, myrank, npoints, npointslocal, i;\n",
    "  double delta, add, addlocal, x;\n",
    "\n",
    "  MPI_Init( &argc, &argv );\n",
    "  MPI_Comm_size( MPI_COMM_WORLD, &size );\n",
    "  MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n",
    "\n",
    "  if(myrank == master)\n",
    "  {\n",
    "    npoints = 1000;\n",
    "    printf(\"\\nNumbers of divide points (%d):\\n\",npoints);\n",
    "  }\n",
    "\n",
    "  MPI_Bcast(&npoints, 1, MPI_INT, master, MPI_COMM_WORLD);\n",
    "\n",
    "  delta = 1.0/((double) npoints);\n",
    "  npointslocal = npoints/size;\n",
    "\n",
    "  printf(\"===================> %ld %ld %ld\\n\", myrank, npoints, npointslocal);\n",
    "\n",
    "  addlocal = 0;\n",
    "\n",
    "  x = myrank * npointslocal * delta;\n",
    "\n",
    "  for(i = 1; i <= npointslocal; ++i)\n",
    "  {\n",
    "    addlocal = addlocal + 1.0/(1+x*x);\n",
    "    x = x + delta;\n",
    "  }\n",
    "\n",
    "  MPI_Reduce(&addlocal, &add, 1, MPI_DOUBLE, MPI_SUM, master, MPI_COMM_WORLD);\n",
    "    \n",
    "  if(myrank == master)\n",
    "  {\n",
    "     add = 4.0 * delta * add;\n",
    "     printf(\"Pi = %20.16lf\\n\", add);\n",
    "  }\n",
    " \n",
    "  MPI_Finalize(); \n",
    "  \n",
    "  return 0;  \n",
    "}   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: AllReduce on Multi-GPU Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the definition of the __ncclAllReduce__ function, write a parallel program using NCCL that makes use of the collective communication function of `AllReduce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "     ncclAllReduce(const void* sendbuff,           \n",
    "                   void* recvbuff,                            \n",
    "                   size_t count,                              \n",
    "                   ncclDataType_t datatype,                   \n",
    "                   ncclRedOp_t op,                                                              \n",
    "                   ncclComm_t comm,                           \n",
    "                   cudaStream_t stream                      \n",
    "                  );\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Matrix-Vector Multiply on Multi-GPU Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a parallel program on multi-GPU systems that executes a matrix-vector product $y=Ax+b$  using a data distribution where the matrix $A$ and the result vector $y$ are distributed in blocks by rows of size $b$, and the vector $x$ is spread in its entirety to all computational resources. Once the partial products have been made, the computational resource $P_i$ should only have the $y_i$ block of $y$. The simplest way to perform this operation is through the `AllGather` operation of the block stored in all resources in the group. Below is the MPI solution for $n$ elements of the vector and distributed among all the resources $y_{distr}$ of the group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "void function(int n, double *y_distr, double *y) \n",
    "{ \n",
    " int P;                             //number of resources 'P'\n",
    " MPI_Comm_size(MPI_COMM_WORLD, &P); //Getting the number of features 'P'\n",
    " send_count =  n / P;               //send_count' generates the value of 'n' and 'P'\n",
    "\n",
    " MPI_Allgather(y_distr, send_count, MPI_DOUBLE, Y, send_count, MPI_DOUBLE, MPI_COMM_WORLD); \n",
    "}  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear the Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, please execute the following cell to clear up the CPU memory. This is required to move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook we will look at CUDA-aware MPI which will give us the benefits of the SPMD programming model [_3-SDumont-NCCL-P2P.ipynb_](3-SDumont-NCCL-P2P.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
