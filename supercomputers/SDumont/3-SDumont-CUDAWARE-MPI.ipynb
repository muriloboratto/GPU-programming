{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDAWARE-MPI on Multi-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will introduce how MPI and CUDA compatibility works, how efficient it is, and how it can be used on API CUDAWARE-MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understand the concepts of MPI, CUDA and CUDAWARE-MPI on multiple GPUs.\n",
    "- Understant the API CUDAWARE-MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks Ping-Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will look at a simple *ping pong* code that measures the bandwidth for data transfers between 2 MPI processes. We will look at the following versions:\n",
    "\n",
    "- A first version using CPU with __MPI__;\n",
    "- A second version with __MPI + CUDA__ between two GPUs which processes data through CPU memory;\n",
    "- And the last one that uses __CUDAWARE-MPI__ which exchange data directly between GPUs using GPUdirect or by NVLINK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by looking at a CPU-only version of the code to understand the idea behind a simple data transfer program (*ping-pong*). MPI processes pass data back and forth, and bandwidth is calculated by measuring the data transfers, as you know how much size is being transferred. Let is look at the `ping-pong-MPI.c` code to see how it is implemented. At the top of the main program, we start the MPI, determine the total number of processes and the rank identifiers, and make sure we only have two ranks in total to run the *ping-pong*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    int size, rank;\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Status status; \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then enter the main *loop* `for`, where each iteration performs data transfers and bandwidth calculations for different message size, ranging from 8 bytes to 1 GB:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "   for(int i = 0; i <= 27; i++)\n",
    "     long int N = 1 << i; \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the *A* array, define some labels to match the MPI send/receive pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "   double *A = (double*) calloc (N, sizeof(double)); \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, each iteration of the *loop* does the following:\n",
    "\n",
    "- If rank is 0, it first sends a message with data from the matrix \\verb+A+ to rank 1, then expects to receive a message of rank 1.\n",
    "\n",
    "- If rank is 1, first expect to receive a message from rank 0 and then send a message back to rank 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    start_time = MPI_Wtime();\n",
    "    for(int i = 1; i <= loop_count; i++)\n",
    "    {\n",
    "      if(rank == 0)\n",
    "      {\n",
    "        MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "        MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &stat);\n",
    "      }else if(rank == 1)\n",
    "       {\n",
    "         MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &stat);\n",
    "         MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "       }\n",
    "    }\n",
    "    stop_time = MPI_Wtime();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous two points describe an application data transfer *ping-pong*. Now that we are familiar with the basic *ping-pong* code in MPI let us look at a version that includes GPUs with CUDA. In this example, we are still passing data back and forth between two MPI ratings, but the data is in GPU memory this time. More specifically, rank 0 has a memory buffer on GPU 0, and rank 1 has a memory buffer on GPU 1, and they will pass the data between the memories of the two GPUs. Here, to get data from memory from GPU 0 to GPU 1, we will first put the data into CPU memory *host*. Next, we can see the differences between the previous version to the new version with MPI+CUDA. Then, from the synchronization results and the known size of the data transfers, we calculate the bandwidth and print the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "long int num_B = 8 * N;\n",
    "long int B_in_GB = 1 << 30;\n",
    "double num_GB = (double)num_B / (double)B_in_GB;\n",
    "double avg_time_per_transfer=elapsed_time/(2.0*(double)loop_count);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in order to compile MPI programs, we must include the appropriate compilation option, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ping-pong-MPI.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <unistd.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    int size, rank;\n",
    "\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    MPI_Status status;\n",
    "\n",
    "    double start_time, stop_time, elapsed_time;\n",
    "       \n",
    "    for(int i = 0; i <= 27; i++) \n",
    "    {\n",
    "       long int N = 1 << i; /*Loop from 8 Bytes to 1 GB*/\n",
    "\n",
    "       double *A = (double*)calloc( N, sizeof(double));  /*Allocate memory for A on CPU*/\n",
    "\n",
    "       int tag1 = 1000;\n",
    "       int tag2 = 2000;\n",
    "\n",
    "       int loop_count = 50;\n",
    "\n",
    "       /********************************/      \n",
    "       /**/ start_time = MPI_Wtime();/**/\n",
    "       /********************************/\n",
    "\n",
    "       for(int i = 1; i <= loop_count; i++)\n",
    "       {\n",
    "            if(rank == 0)\n",
    "            {\n",
    "               MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "               MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &status);\n",
    "            }\n",
    "            else if(rank == 1)\n",
    "            {\n",
    "               MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n",
    "               MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "            }\n",
    "        }\n",
    "\n",
    "       /*********************************/      \n",
    "       /**/  stop_time = MPI_Wtime(); /**/\n",
    "       /********************************/      \n",
    "      \n",
    "        /*measured time*/\n",
    "        elapsed_time = stop_time - start_time;  \n",
    "        long int num_B = 8 * N;\n",
    "        long int B_in_GB = 1 << 30;\n",
    "        double num_GB = (double)num_B / (double)B_in_GB;\n",
    "        double avg_time_per_transfer = elapsed_time / (2.0*(double)loop_count);\n",
    "\n",
    "        if(rank == 0) \n",
    "            printf(\"Transfer size (Bytes): %10li, Transfer Time (seconds): %15.9f, Bandwidth (GB/s): %15.9f\\n\", \n",
    "                   num_B, avg_time_per_transfer, num_GB/avg_time_per_transfer );  \n",
    "\n",
    "        free(A);   \n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " module load openmpi/gnu/4.1.4+cuda-11.2\n",
    " mpicc ping-pong-MPI.c -o ping-pong-MPI\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Slurm-MPI.sh\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=MPI                         # Job name\n",
    "#SBATCH --nodes=2                              # Run all processes on 2 nodes  \n",
    "#SBATCH --partition=sequana_gpu_dev            # Partition SDUMONT\n",
    "#SBATCH --output=out_v100_%j-MPI.log           # Standard output and error log\n",
    "#SBATCH --ntasks-per-node=1                    # 1 job per node\n",
    "\n",
    "module load openmpi/gnu/4.1.4+cuda-11.2\n",
    "mpirun -np 2 ./ping-pong-MPI+CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " sbatch slurm-MPI+CUDA.sh\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtoexecute.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print output in log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat *-MPI.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MPI + CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with the basic *ping-pong* code in MPI let us look at a version that includes GPUs with CUDA. In this example, we are still passing data back and forth between two MPI ratings, but the data is in GPU memory this time. More specifically, rank 0 has a memory buffer on GPU 0, and rank 1 has a memory buffer on GPU 1, and they will pass the data between the memories of the two GPUs. Here, to get data from memory from GPU 0 to GPU 1, we will first put the data into CPU memory *host*. Next, we can see the differences between the previous version to the new version with MPI+CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    " start_time = MPI_Wtime();\n",
    " for(int i = 1; i <= loop_count; i++)\n",
    " {\n",
    "  if(rank == 0)\n",
    "  {\n",
    "   cudaMemcpy(A, d_A, N * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "   MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "   MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &status);\n",
    "   cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "   }else if(rank == 1)\n",
    "    {\n",
    "     MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n",
    "     cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "     cudaMemcpy(A, d_A, N * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "     MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "    }\n",
    " }\n",
    " stop_time = MPI_Wtime();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the CPU-only version, we initialize MPI and find the identifier of each MPI rank, but here we also assign each rank a different GPU (i.e., rank 0 is assigned to GPU 0 and rank 1 is mapped to GPU 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "int size, rank;\n",
    "MPI_Init(&argc, &argv);\n",
    "MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "MPI_Status status;\n",
    "\n",
    "cudaSetDevice(rank);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this release, each iteration of the \\emph{loop} does the following:\n",
    "\n",
    "- We enter the main *loop* `for`, which iterates over the different message sizes, and assign and initialize the __A__ array. However, we now have a call to `cudaMalloc` to reserve a memory buffer __d_A__ on the GPUs and a call to `cudaMemcpy` to transfer the data initialized in the *A* array to the buffer __d_A__. We need the command `cudaMemcpy` to get the data to the GPU before we start our *ping-pong*.\n",
    "\n",
    "- Data must first be transferred from GPU memory 0 to CPU memory. Then an MPI call is used to pass the data from ranks 0 to 1. Now that rank 1 has the data (in CPU memory), it can transfer it to GPU memory 1. Rank 0 must first transfer the data from a buffer in GPU 0 memory to one in CPU memory. Now that rank 1 contains the data in the CPU memory buffer, and it can transfer it to GPU 1 memory.\n",
    "\n",
    "As in the case where only the CPU is used, from the synchronization results and the known size of the data transfers, we calculate the bandwidth, print the results, and finally free up the memory of the computational resources. We ended the MPI and the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ping-pong-MPI+CUDA.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda.h>\n",
    "#include <unistd.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    int size, rank;\n",
    "\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    MPI_Status status;\n",
    "\n",
    "    cudaSetDevice(rank);\n",
    "\n",
    "    double start_time, stop_time, elapsed_time;\n",
    "\n",
    "    for(int i = 0; i <= 27; i++)\n",
    "    {\n",
    "        long int N = 1 << i; /*Loop from 8 Bytes to 1 GB*/\n",
    "   \n",
    "        double *A = (double*)calloc(N, sizeof(double)); /*Allocate memory for A on CPU*/\n",
    "\n",
    "        double *d_A;\n",
    "\n",
    "        cudaMalloc(&d_A, N * sizeof(double)) ;\n",
    "        cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "\n",
    "        int tag1 = 1000;\n",
    "        int tag2 = 2000;\n",
    "\n",
    "        int loop_count = 50;\n",
    "\n",
    "       /********************************/      \n",
    "       /**/ start_time = MPI_Wtime();/**/\n",
    "       /********************************/\n",
    "\n",
    "        for(int i = 1; i <= loop_count; i++)\n",
    "        {\n",
    "            if(rank == 0)\n",
    "            {\n",
    "                cudaMemcpy(A, d_A, N * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "                MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "                MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &status);\n",
    "                cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "            }\n",
    "            else if(rank == 1)\n",
    "            {\n",
    "                MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n",
    "                cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "                cudaMemcpy(A, d_A, N * sizeof(double), cudaMemcpyDeviceToHost);\n",
    "                MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "            }\n",
    "        }\n",
    "\n",
    "       /**********************************/      \n",
    "       /**/  stop_time = MPI_Wtime(); /**/\n",
    "       /*********************************/\n",
    "\n",
    "        /*measured time*/\n",
    "        elapsed_time = stop_time - start_time;\n",
    "        long int num_B = 8 * N;\n",
    "        long int B_in_GB = 1 << 30;\n",
    "        double num_GB = (double)num_B / (double)B_in_GB;\n",
    "        double avg_time_per_transfer = elapsed_time / (2.0*(double)loop_count);\n",
    "\n",
    "        if(rank == 0) \n",
    "          printf(\"Transfer size (Bytes): %10li, Transfer Time (seconds): %15.9f, Bandwidth (GB/s): %15.9f\\n\", \n",
    "                    num_B, avg_time_per_transfer, num_GB/avg_time_per_transfer );\n",
    "\n",
    "        cudaFree(d_A);\n",
    "        free(A);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " module load openmpi/gnu/4.1.4+cuda-11.2\n",
    " nvcc $CPPFLAGS $LDFLAGS -lmpi ping-pong-MPI+CUDA.cu -o ping-pong-MPI+CUDA   \n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Slurm-MPI+CUDA.sh\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=MPI+CUDA                    # Job name\n",
    "#SBATCH --nodes=2                              # Run all processes on 2 nodes  \n",
    "#SBATCH --partition=sequana_gpu_dev            # Partition SDUMONT\n",
    "#SBATCH --output=out_v100_%j-MPI+CUDA.log      # Standard output and error log\n",
    "#SBATCH --ntasks-per-node=1                    # 1 job per node\n",
    "\n",
    "module load openmpi/gnu/4.1.4+cuda-11.2\n",
    "mpirun -np 2 --report-bindings --map-by numa -x UCX_MEMTYPE_CACHE=n -mca pml ucx -mca btl ^vader,tcp,openib,smcuda -x UCX_NET_DEVICES=mlx4_0:1 ./ping-pong-MPI+CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " sbatch slurm-MPI+CUDA.sh\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtoexecute.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print output in log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat *-MPI+CUDA.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CUDAWARE-MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at this code example, let us first describe [CUDAWARE-MPI](https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/) and [GPUDirect RDMA](https://docs.nvidia.com/cuda/gpudirect-rdma/index.html). CUDAWARE-MPI is an MPI implementation that allows GPU buffers (e.g., GPU memory allocated with cudaMalloc) to be used directly in MPI calls. However, CUDAWARE-MPI alone does not specify whether data is stored in intermediate stages in CPU memory or passed from GPU to GPU. It will depend on the computational structure of the execution environment.\n",
    "\n",
    "The GPUDirect is an umbrella name used to refer to several specific technologies. In MPI, the GPUDirect technologies cover all kinds of inter-rank communication: intra-node, inter-node, and RDMA inter-node communication. Now let us take a look at the code below. It is the same as the tested version of MPI+CUDA, but now there are no calls to cudaMemcpy during the ping-pong steps. Instead, we use our GPU buffers (__d_A__) directly in MPI calls:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "    start_time = MPI_Wtime();\n",
    "    for(int i = 1; i <= loop_count; i++)\n",
    "    {\n",
    "      if(rank == 0)\n",
    "      {\n",
    "        MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "        MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &stat);\n",
    "      }else if(rank == 1)\n",
    "       {\n",
    "         MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &stat);\n",
    "         MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "       }\n",
    "    }\n",
    "    stop_time = MPI_Wtime();\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ping-pong-CUDAWARE-MPI.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <cuda.h>\n",
    "#include <unistd.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    int size, rank;\n",
    "\n",
    "    MPI_Init(&argc, &argv);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "\n",
    "    MPI_Status status;\n",
    "\n",
    "    cudaSetDevice(rank);\n",
    "\n",
    "    double start_time, stop_time, elapsed_time;\n",
    "\n",
    "    for(int i = 0; i <= 27; i++)\n",
    "    {\n",
    "        long int N = 1 << i; /*Loop from 8 Bytes to 1 GB*/\n",
    "   \n",
    "        double *A = (double*)calloc(N, sizeof(double)); /*Allocate memory for A on CPU*/\n",
    "\n",
    "        double *d_A;\n",
    "\n",
    "        cudaMalloc(&d_A, N * sizeof(double)) ;\n",
    "        cudaMemcpy(d_A, A, N * sizeof(double), cudaMemcpyHostToDevice);\n",
    "\n",
    "        int tag1 = 1000;\n",
    "        int tag2 = 2000;\n",
    "\n",
    "        int loop_count = 50;\n",
    "\n",
    "       /********************************/      \n",
    "       /**/ start_time = MPI_Wtime();/**/\n",
    "       /********************************/\n",
    "\n",
    "        for(int i = 1; i <= loop_count; i++)\n",
    "        {\n",
    "            if(rank == 0)\n",
    "            {\n",
    "              MPI_Send(d_A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n",
    "              MPI_Recv(d_A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &status);\n",
    "            }\n",
    "            else if(rank == 1)\n",
    "            {\n",
    "              MPI_Recv(d_A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n",
    "              MPI_Send(d_A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n",
    "            }\n",
    "         }\n",
    "\n",
    "       /**********************************/      \n",
    "       /**/  stop_time = MPI_Wtime(); /**/\n",
    "       /*********************************/\n",
    "\n",
    "        /*measured time*/\n",
    "        elapsed_time = stop_time - start_time;\n",
    "        long int num_B = 8 * N;\n",
    "        long int B_in_GB = 1 << 30;\n",
    "        double num_GB = (double)num_B / (double)B_in_GB;\n",
    "        double avg_time_per_transfer = elapsed_time / (2.0*(double)loop_count);\n",
    "\n",
    "        if(rank == 0) \n",
    "            printf(\"Transfer size (Bytes): %10li, Transfer Time (seconds): %15.9f, Bandwidth (GB/s): %15.9f\\n\", \n",
    "                    num_B, avg_time_per_transfer, num_GB/avg_time_per_transfer );\n",
    "\n",
    "        cudaFree(d_A);\n",
    "        free(A);\n",
    "    }\n",
    "\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh sdumont\"\n",
    "}\n",
    "\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " module load openmpi/gnu/4.1.4+cuda-11.2\n",
    " nvcc $CPPFLAGS $LDFLAGS -lmpi ping-pong-CUDAWARE-MPI.cu -o ping-pong-CUDAWARE-MPI\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " ogbon\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Slurm-CUDAWARE-MPI.sh\n",
    "#!/bin/sh\n",
    "\n",
    "#SBATCH --job-name=CUDA-AWARE-MPI              # Job name\n",
    "#SBATCH --nodes=2                              # Run all processes on 2 nodes  \n",
    "#SBATCH --partition=sequana_gpu_dev            # Partition SDUMONT\n",
    "#SBATCH --output=out_v100_%j-CUDAWARE-MPI.log  # Standard output and error log\n",
    "#SBATCH --ntasks-per-node=1                    # 1 job per node\n",
    "\n",
    "module load openmpi/gnu/4.1.4+cuda-11.2\n",
    "mpirun -np 2 --report-bindings --map-by numa -x UCX_MEMTYPE_CACHE=n --map-by numa --report-bindings -mca pml ucx -mca btl ^vader,tcp,openib,smcuda ./ping-pong-CUDAWARE-MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " sbatch slurm-CUDAWARE-MPI.sh\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtoexecute.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print output in log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat *-CUDAWARE-MPI.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Comparison Performance internode: NCCL x CUDA-aware MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the following ping-pong code using NCCL within one compute node with the previous implementation of CUDA-aware MPI. The idea is to understand why the values differ since both pass through the same high-speed channel inside the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ping-pong-NCCL.cu\n",
    "#include <iostream>\n",
    "#include <nccl.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <chrono>\n",
    "\n",
    "#define NUM_GPUS 2\n",
    "\n",
    "__global__ void print_values(int gpu_id, float *data) {\n",
    "  printf(\"GPU %d: %f\\n\", gpu_id, data[threadIdx.x]);\n",
    "}\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "  ncclComm_t comms[NUM_GPUS];\n",
    "\n",
    "  cudaStream_t streams[NUM_GPUS];\n",
    "\n",
    "  // Initializing NCCL\n",
    "  ncclUniqueId id;\n",
    "  ncclGetUniqueId(&id);\n",
    "  ncclGroupStart();\n",
    "  for (int i = 0; i < NUM_GPUS; ++i) {\n",
    "    cudaSetDevice(i);\n",
    "    ncclCommInitRank(&comms[i], NUM_GPUS, id, i);\n",
    "  }\n",
    "  ncclGroupEnd();\n",
    "\n",
    "  // Create a stream on each GPU\n",
    "  for (int i = 0; i < NUM_GPUS; ++i) {\n",
    "    cudaSetDevice(i);\n",
    "    cudaStreamCreate(&streams[i]);\n",
    "  }\n",
    "\n",
    "  for (int i = 0; i <= 27; i++) {\n",
    "    long int N = 1 << i;\n",
    "    size_t numBytes = N * sizeof(float);\n",
    "    float *buffers[NUM_GPUS];\n",
    "\n",
    "    // Allocate memory on each GPU\n",
    "    for (int j = 0; j < NUM_GPUS; ++j) {\n",
    "      cudaSetDevice(j);\n",
    "      cudaMalloc(&buffers[j], numBytes);\n",
    "    }\n",
    "\n",
    "    // Initializing data on each GPU\n",
    "    for (int j = 0; j < NUM_GPUS; ++j) {\n",
    "      cudaSetDevice(j);\n",
    "      float *h_data = new float[N];\n",
    "      for (int k = 0; k < N; ++k) h_data[k] = j + 1.0f;\n",
    "      cudaMemcpy(buffers[j], h_data, numBytes, cudaMemcpyHostToDevice);\n",
    "      delete[] h_data;\n",
    "    }\n",
    "\n",
    "    int loop_count = 50;\n",
    "\n",
    "    // Performing ping-pong between GPUs and measuring time\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaSetDevice(0);\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    cudaEventRecord(start, streams[0]);\n",
    "\n",
    "    for (int j = 0; j < loop_count; ++j) {\n",
    "      int src = j % NUM_GPUS;\n",
    "      int dst = (j + 1) % NUM_GPUS;\n",
    "\n",
    "      ncclGroupStart();\n",
    "      cudaSetDevice(src);\n",
    "      ncclSend(buffers[src], N, ncclFloat, dst, comms[src], streams[src]);\n",
    "\n",
    "      cudaSetDevice(dst);\n",
    "      ncclRecv(buffers[dst], N, ncclFloat, src, comms[dst], streams[dst]);\n",
    "      ncclGroupEnd();\n",
    "    }\n",
    "\n",
    "    cudaEventRecord(stop, streams[0]);\n",
    "    cudaEventSynchronize(stop);\n",
    "\n",
    "    float elapsedTime;\n",
    "    cudaEventElapsedTime(&elapsedTime, start, stop);\n",
    "    \n",
    "    /*measured*/\n",
    "    long int num_B = 8 * N;\n",
    "    long int B_in_GB = 1 << 30;\n",
    "    double num_GB = (double)num_B / (double)B_in_GB;\n",
    "    double avg_time_per_transfer = (elapsedTime * 1e-3) / (2.0*(double)loop_count);\n",
    "    float bandwidth = num_GB/avg_time_per_transfer ;\n",
    "  \n",
    "    printf(\"Transfer size (Bytes): %10li, Transfer Time (seconds): %15.9f, Bandwidth (GB/s): %15.9f\\n\", \n",
    "                  num_B, avg_time_per_transfer, bandwidth  );\n",
    " \n",
    "    // Cleanup memory\n",
    "    for (int j = 0; j < NUM_GPUS; ++j) {\n",
    "      cudaSetDevice(j);\n",
    "      cudaFree(buffers[j]);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Destroy NCCL communicators\n",
    "  for (int i = 0; i < NUM_GPUS; ++i) \n",
    "  {\n",
    "    cudaSetDevice(i);\n",
    "    ncclCommDestroy(comms[i]);\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh sdumont\"\n",
    "}\n",
    "\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " module load nccl/2.13_cuda-11.2\n",
    " nvcc $CPPFLAGS $LDFLAGS ping-pong-NCCL.cu -o ping-pong-NCCL -lnccl -std=c++11\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " ogbon\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Slurm-NCCL.sh\n",
    "#!/bin/sh\n",
    "\n",
    "#SBATCH --job-name=NCCL                        # Job name\n",
    "#SBATCH --nodes=1                              # Run all processes on 2 nodes  \n",
    "#SBATCH --partition=sequana_gpu_dev            # Partition SDUMONT\n",
    "#SBATCH --output=out_v100_%j-NCCL.log          # Standard output and error log\n",
    "#SBATCH --ntasks-per-node=1                    # 1 job per node\n",
    "\n",
    "module load nccl/2.13_cuda-11.2\n",
    "./ping-pong-NCCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " sbatch slurm-NCCL.sh\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtoexecute.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print output in log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat *-NCCL.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear the Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, please execute the following cell to clear up the CPU memory. This is required to move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please continue to the next notebook: [_4-SDumont-MCπ-SGPU.ipynb_](4-SDumont-MCπ-SGPU.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
