{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Case Study: Monte Carlo Approximation of $\\pi$ - CUDAWARE-MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will introduce CUDAWARE-MPI, which will grant us the benefits of the SPMD paradigm while retaining the ability to utilize direct peer-to-peer memory with multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Be able to use CUDAWARE-MPI to run multiple copies of a CUDA application on multiple GPUs while leveraging direct peer-to-peer memory access between GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDAWARE-MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPI helped clean up much of the boilerplate we used when managing multiple devices explicitly. But we also gave up the benefit of multiple GPUs talking to each other directly. MPI is a [distributed memory parallel programming model](https://en.wikipedia.org/wiki/Distributed_memory), where each processor has its own (virtual) memory and address space, even if all ranks are on the same server and thus share the same physical memory. (This is typically contrasted with [shared memory parallel programming models](https://en.wikipedia.org/wiki/Shared_memory), where each processing thread has access to the same memory space, like [OpenMP](https://en.wikipedia.org/wiki/OpenMP), and also like traditional single-GPU CUDA programming where all threads have access to global memory.) So we copied the result for each GPU to the CPU and then summed the results on the CPU.\n",
    "\n",
    "But as long as we're staying on a single server the rules of the [CUDA unified virtual address space](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#unified-virtual-address-space) still hold, so all *CUDA* allocations result in virtual addresses that can be meaningfully shared across processes (even if the normal CPU dynamic memory allocations cannot be). As a result, it's possible for MPI to directly implement peer memory copies under the hood. For communication among remote servers this is not possible, but there are other technologies that allow direct GPU to GPU communication through a network interface, in particular [GPUDirect RDMA](https://docs.nvidia.com/cuda/gpudirect-rdma/index.html). Recognizing the value in leveraging these technologies for efficient communication, many MPI implementations (including OpenMPI, which we are using in this workshop) provide [CUDAWARE-MPI](https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/), which allows the programmer to provide an address to an MPI communication routine which may reside on a device. The MPI implementation is then free to use whatever scheme it desires to transmit the data from one GPU to another, including the use of GPUDirect P2P and GPUDirect RDMA where appropriate. (Note that [GPUDirect](https://developer.nvidia.com/gpudirect) refers to a family of technologies while CUDAWARE-MPI refers to an API which may use those technologies under the hood, although it is common to see the two terms incorrectly conflated.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/GPUDirectRDMA.png\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So CUDAWARE-MPI provides the benefit of simplified programming while retaining the performance benefit of avoiding unnecessary copies to CPU memory. With that in mind, one way to write the final reduction is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "MPI_Reduce(d_hits, total_hits, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where MPI automatically detects that the send buffer `d_hits` resides on the device while the receive buffer `total_hits` resides on the host and does the right thing behind the scenes to enable this copy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Reduction Entirely on GPUs with CUDAWARE-MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, rewrite this application to do the reduction entirely in GPU memory.\n",
    "\n",
    "In order to do this you will need to create a device array for storing the hits total, and use this device array in the call to `MPI_Reduce`, and then explicitly copy the result back to the host on rank 0 at the end.\n",
    "\n",
    "You can check the solution in the code `monte_carlo_mgpu_cuda_mpi_cuda_aware.cu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile monte_carlo_mgpu_cuda_mpi_cuda_aware.cu\n",
    "#include <iostream>\n",
    "#include <curand_kernel.h>\n",
    "#include <mpi.h>\n",
    "\n",
    "#define N 1024*1024\n",
    "\n",
    "__global__ void calculate_pi(int* hits, int device) \n",
    "{\n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    // Initialize random number state (unique for every thread in the grid)\n",
    "    int seed = device;\n",
    "    int offset = 0;\n",
    "    curandState_t curand_state;\n",
    "    curand_init(seed, idx, offset, &curand_state);\n",
    "\n",
    "    // Generate random coordinates within (0.0, 1.0]\n",
    "    float x = curand_uniform(&curand_state);\n",
    "    float y = curand_uniform(&curand_state);\n",
    "\n",
    "    // Increment hits counter if this point is inside the circle\n",
    "    if (x * x + y * y <= 1.0f) \n",
    "        atomicAdd(hits, 1);\n",
    "                                           \n",
    "}\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    // Initialize MPI\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    // Obtain our rank and the total number of ranks\n",
    "    // MPI_COMM_WORLD means that we want to include all processes\n",
    "    // (it is possible in MPI to create \"communicators\" that only\n",
    "    // include some of the ranks).\n",
    "\n",
    "    int rank, num_ranks;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n",
    "\n",
    "    // Ensure that we don't have more ranks than GPUs\n",
    "    int device_count;\n",
    "    cudaGetDeviceCount(&device_count);\n",
    "\n",
    "    if (num_ranks > device_count) \n",
    "    {\n",
    "        std::cout << \"Error: more MPI ranks than GPUs\" << std::endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // Each rank (arbitrarily) chooses the GPU corresponding to its rank\n",
    "    int dev = rank;\n",
    "    cudaSetDevice(dev);\n",
    "\n",
    "    // Allocate host and device values\n",
    "    int* hits;\n",
    "    hits = (int*) malloc(sizeof(int));\n",
    "\n",
    "    int* d_hits;\n",
    "    cudaMalloc((void**) &d_hits, sizeof(int));\n",
    "\n",
    "    // Initialize number of hits and copy to device\n",
    "    *hits = 0;\n",
    "    cudaMemcpy(d_hits, hits, sizeof(int), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch kernel to do the calculation\n",
    "    int threads_per_block = 256;\n",
    "    int blocks = (N / device_count + threads_per_block - 1) / threads_per_block;\n",
    "\n",
    "    calculate_pi<<<blocks, threads_per_block>>>(d_hits, dev);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Accumulate the results across all ranks to the result on rank 0\n",
    "    int* d_total_hits;\n",
    "    cudaMalloc((void**) &d_total_hits, sizeof(int));\n",
    "\n",
    "    int root = 0;\n",
    "    MPI_Reduce(d_hits, d_total_hits, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);\n",
    "\n",
    "    if (rank == root) \n",
    "    {\n",
    "        // Copy result back to host\n",
    "        int* total_hits = (int*) malloc(sizeof(int));\n",
    "        cudaMemcpy(total_hits, d_total_hits, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "\n",
    "        // Calculate final value of pi\n",
    "        float pi_est = (float) *total_hits / (float) (N) * 4.0f;\n",
    "        free(total_hits);\n",
    "\n",
    "        // Print out result\n",
    "        std::cout << \"Estimated value of pi = \" << pi_est << std::endl;\n",
    "        std::cout << \"Error = \" << std::abs((M_PI - pi_est) / pi_est) << std::endl;\n",
    "    }\n",
    "\n",
    "    // Clean up\n",
    "    free(hits);\n",
    "    cudaFree(d_hits);\n",
    "\n",
    "    // Finalize MPI\n",
    "    MPI_Finalize();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile with Shell Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtocompile.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtocompile.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtocompile.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtocompile.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " module load openmpi/gnu/4.1.4+cuda-11.2\n",
    " nvcc $CPPFLAGS $LDFLAGS -lmpi -ccbin=mpicxx monte_carlo_mgpu_cuda_mpi_cuda_aware.cu -o monte_carlo_mgpu_cuda_mpi_cuda_aware\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " ogbon\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtocompile.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute with Shell Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Slurm-MONTECARLO-CUDAWARE-MPI.sh\n",
    "#!/bin/sh\n",
    "\n",
    "#SBATCH --job-name=MONTECARLO-CUDAWARE-MPI                # Job name\n",
    "#SBATCH --nodes=2                                         # Run all processes on 2 nodes  \n",
    "#SBATCH --partition=sequana_gpu_dev                       # Partition SDUMONT\n",
    "#SBATCH --output=out_v100_%j-MONTECARLO-CUDAWARE-MPI.log  # Standard output and error log\n",
    "#SBATCH --ntasks-per-node=1                               # 1 job per node\n",
    "\n",
    "module load openmpi/gnu/4.1.4+cuda-11.2\n",
    "mpirun -np 2 --report-bindings --map-by numa -x UCX_MEMTYPE_CACHE=n -mca pml ucx -mca btl ^vader,tcp,openib,smcuda -x UCX_NET_DEVICES=mlx5_0:1 ./monte_carlo_mgpu_cuda_mpi_cuda_aware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile howtoexecute.sh\n",
    "#!/bin/bash\n",
    "\n",
    "usage()\n",
    "{\n",
    " echo \"howtoexecute.sh: wrong number of input parameters. Exiting.\"\n",
    " echo -e \"Usage: bash howtoexecute.sh <supercomputer>\"\n",
    " echo -e \"  g.e: bash howtoexecute.sh sdumont\"\n",
    "}\n",
    "\n",
    "sdumont()\n",
    "{\n",
    " sbatch slurm-CUDAWARE-MPI.sh\n",
    "}\n",
    "\n",
    "#args in comand line\n",
    "if [ \"$#\" ==  0 ]; then\n",
    " usage\n",
    " exit\n",
    "fi\n",
    "\n",
    "#sdumont\n",
    "if [[ $1 == \"sdumont\" ]];then\n",
    " sdumont\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash howtoexecute.sh sdumont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print output in log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat *-MONTECARLO-CUDAWARE-MPI.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear the Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, please execute the following cell to clear up the CPU memory. This is required to move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please continue to the next notebook: [_7-SDumont-MCπ-NVSHMEM.ipynb_](7-SDumont-MCπ-NVSHMEM.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
